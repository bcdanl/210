---
title: Classwork 4
subtitle: Scrapping Data with Python `selenium`
date: 2026-02-18
execute: 
  eval: false
  echo: false
  warning: false
  message: false
  
from: markdown+emoji
---


```{r}
#| include: false

library(tidyverse)
library(DT)
library(reticulate)
reticulate::use_condaenv("/Users/bchoe/anaconda3", required = TRUE)
```


<div style="text-align: center; width: 75%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/giphy-hacking-start.gif" style="width: 100%; margin-bottom: 0px;">
  <p style="font-weight: bold;"></p>
</div>

<br>


Below is to set up the web scrapping environment with Python `selenium`:

```{.python}
import pandas as pd
import os, time, random
from io import StringIO

# Import the necessary modules from the Selenium library
from selenium import webdriver  # Main module to control the browser
from selenium.webdriver.common.by import By  # Helps locate elements on the webpage
from selenium.webdriver.chrome.options import Options  # Allows setting browser options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import StaleElementReferenceException

# Set the working directory path
wd_path = 'ABSOLUTE_PATHNAME_OF_YOUR_WORKING_DIRECTORY' # e.g., '/Users/bchoe/Documents/DANL-210'
os.chdir(wd_path)  # Change the current working directory to wd_path
os.getcwd()  # Retrieve and return the current working directory

# Create an instance of Chrome options
options = Options()

# Initialize the Chrome WebDriver with the specified options
driver = webdriver.Chrome(options=options)
```

<br>


# Collecting Rows into a pandas `DataFrame`

```{.python}
df = pd.DataFrame()

for i in range(1, 6):
    
    obs_lst = [i, "Ava", "Geneseo", 2025]
    
    obs = pd.DataFrame([obs_lst])
    df = pd.concat([df, obs], ignore_index = True)
    
df.columns = ['id', 'name', 'school', 'year']
```

1. **Start with an empty `DataFrame`**
- `df = pd.DataFrame()` creates an empty table (no rows yet).

2. **Repeat the same process five times**
- `for i in range(1, 6):` runs the indented block **5 times** (for `i = 1, 2, 3, 4, 5`).

3. **Create one ‚Äúobservation‚Äù as a list**
- `obs_lst = [i, "Ava", "Geneseo", 2026]` is a Python list with **3 values**.
- Think of this list as **one row of data** you want to add.

4. **Turn the list into a one-row `DataFrame`**
- `obs = pd.DataFrame([obs_lst])`
- Wrapping `obs_lst` with brackets (`[obs_lst]`) makes it a **list of rows** (2D), so pandas creates a **1-row DataFrame**.
- Because no column names are given, pandas assigns default column labels: `0`, `1`, `2`.

5. **Append the new row to `df` by concatenation**
- `df = pd.concat([df, obs], ignore_index=True)`
- **`pd.concat(...)` stacks DataFrames **row-wise** (adds rows).**
  - The first argument `[df, obs]` for `pd.concat(...)` is a **list** of **DataFrames**.
  - `ignore_index=True` resets the row index to `0, 1, 2, ...` so the final table has clean numbering.

6. **Assign column names to the `DataFrame`**

- `df.columns = ['name', 'school', 'year']` renames the columns of `df`.
- This replaces the default column labels (`0`, `1`, `2`) with meaningful names:
  - `name` for the first column
  - `school` for the second column
  - `year` for the third column
- After this, you can refer to columns using `df['name']`, `df['school']`, and `df['year']`.

## Result
After the loop finishes, `df` contains **five identical rows**:

```{r}
#| echo: false
#| eval: true


rmarkdown::paged_table(
  data.frame(
    id = c(1,2,3,4,5),
    name = rep("Ava", 5),
    school = rep("Geneseo", 5),
    year = rep(2025, 5)
  )
)

```



<br>

# Question 1.
**Task:** Using Selenium and a **single `for` loop**, scrape the table on the EIA page into a **pandas `DataFrame`**.

  - [https://www.eia.gov/petroleum/gasdiesel/gaspump_hist.php](https://www.eia.gov/petroleum/gasdiesel/gaspump_hist.php)
  
### Hints
- Start by scraping the body rows (`tr`) for cell values (`td`)
- You can build XPaths with an f-string inside a loop.

Example idea (XPath f-string):

```{python}
#| eval: false
#| echo: true
for i in range(1, 10):
    xpath = f'//*[@id="main-content"]//table/tbody/tr[{i}]/td[1]'
    print(xpath)
```


*Answer*:
```{python}
#| eval: false
#| echo: true

# TODO: find out the number of rows (<tr>) in the body table (<tbody>)
nrows = 

df = pd.DataFrame()
for i in range(1, nrows + 1):

    # TODO: scrape each cell's text in a single row
    mon_yr = 
    retail_price = 
    refining = 
    distribution_marketing = 
    taxes = 
    crude_oil = 
    
    obs_lst = [mon_yr, retail_price, refining, distribution_marketing, taxes, crude_oil]
    obs = pd.DataFrame([obs_lst])
    
    df = pd.concat([df, obs], ignore_index = True)


```



::: {.callout-tip collapse="true"}
### Show answer

### Web-scraping Strategy with XPath

**Step 0. Set up your environment**

- Import the libraries/modules you need (e.g., Selenium, pandas, time).
- Set your working directory (so your output file saves where you expect).

**Step 1. Find an XPath pattern for the element(s) you want**

- Use your browser‚Äôs Developer Tools (Inspect) to locate the target element.
- Identify what changes across repeated elements (often an index like `[1]`, `[2]`, `[3]`).

**Step 2. Build a dynamic XPath using an f-string**

- Turn the XPath pattern into a template.
- Replace the changing part (usually the index) with `{i}` so you can loop over it.

**Step 3. Write a loop to collect the data**. 
Things to decide before you code:

- **How many times to iterate** (e.g., number of rows, pages, or cards).
- **How to locate elements**
  - `find_element()` for a single element.
  - `find_elements()` for a list of elements.
- **How to store results**
  - Append each observation (row) to a list of dictionaries.
  - Convert the list into a pandas `DataFrame` at the end.

**Step 4. Export the DataFrame to a CSV**

- Use `df.to_csv("filename.csv", index=False)` so the CSV is clean and ready to use.

```{python}
#| echo: true

# %%
# =============================================================================
# Step 0. Set up your environment
# =============================================================================
import pandas as pd
import os, time, random
from io import StringIO

# Import the necessary modules from the Selenium library
from selenium import webdriver  # Main module to control the browser
from selenium.webdriver.common.by import By  # Helps locate elements on the webpage
from selenium.webdriver.chrome.options import Options  # Allows setting browser options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException
from selenium.common.exceptions import TimeoutException
from selenium.common.exceptions import StaleElementReferenceException

# Set the working directory path
wd_path = '/Users/bchoe/Documents/DANL-210' # e.g., '/Users/bchoe/Documents/DANL-210'
os.chdir(wd_path)  # Change the current working directory to wd_path
os.getcwd()  # Retrieve and return the current working directory

# Create an instance of Chrome options
options = Options()

# Initialize the Chrome WebDriver with the specified options
driver = webdriver.Chrome(options=options)


# %%
# =============================================================================
# Scrapping
# =============================================================================
url = 'https://www.eia.gov/petroleum/gasdiesel/gaspump_hist.php'

driver.get(url)


# Step 1. Find an XPath pattern for the element(s) you want
# Mon-yr
# Row 1 
# /html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[1]/td[1]
# Row 2
# /html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[2]/td[1]

# retail price
# /html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[1]/td[2]
# /html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[2]/td[2]



# How many times to iterate
# TODO: find out the number of rows (<tr>) in the body table (<tbody>)
table = driver.find_element(By.TAG_NAME, 'tbody')
rows = table.find_elements(By.TAG_NAME, 'tr')
nrows = len(rows)


# Step 3. Write a loop to collect the data
df = pd.DataFrame()

for i in range(1, nrows + 1):

    # Step 2. Build a dynamic XPath using an f-string
    xpath_mon_yr = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[1]'
    xpath_retail_price = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[2]'
    xpath_refining = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[3]'
    xpath_distribution_marketing = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[4]'
    xpath_taxes = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[5]'
    xpath_crude_oil = f'/html/body/div[1]/div[2]/div/div[4]/div/div[1]/div/table/tbody/tr[{i}]/td[6]'
    
    # How to locate elements
    mon_yr = driver.find_element(By.XPATH, 
                                 xpath_mon_yr).text
    
    retail_price = driver.find_element(By.XPATH, 
                                       xpath_retail_price).text
    refining = driver.find_element(By.XPATH, 
                                   xpath_refining).text
    distribution_marketing = driver.find_element(By.XPATH, 
                                                 xpath_distribution_marketing).text
    taxes = driver.find_element(By.XPATH, 
                                xpath_refining).text
    crude_oil = driver.find_element(By.XPATH, 
                                    xpath_refining).text
    
    # How to store results
    obs_lst = [mon_yr, retail_price, refining, 
               distribution_marketing, taxes, crude_oil]
    obs = pd.DataFrame([obs_lst])
    
    df = pd.concat([df, obs], ignore_index = True)



# Step 4. Export the DataFrame to a CSV
# Column names
df.columns = ['mon_yr', 'retail_price', 'refining', 
              'distribution_marketing', 'taxes', 'crude_oil']


df.to_csv('data/eia_2026_0220', index= False)


```



:::

<br><br>

# Question 2 ‚Äî Scrape with a nested `for` loop

**Task:** Do the same scraping task as Question 1, but this time use a **nested `for` loop**:

- Outer loop: iterate over **rows** (`tr`)
- Inner loop: iterate over **columns** (`td`)
  - In the inner loop, append `value` to the `data` list.


*Answer*:
```{python}
#| eval: false
#| echo: true

# TODO: find out the number of rows (<tr>) and the number of columns (<td>) in each row in the body table (<tbody>)
nrows = 
ncols = 

df = pd.DataFrame()
for i in range(1, nrows + 1):
  
    data = []    # creating an empty list for one row
    
    for j in range(1, ncols + 1):  # Iterate over column positions
        
        # TODO: scrape each cell's text in a single row
        value =
        
        # TODO: append value to the data list

        
    obs = pd.DataFrame([data])
    df = pd.concat([df, obs], ignore_index=True)

```

<br><br>


# Discussion


<font size = "5">Welcome to our Classwork 4 Discussion Board! üëã </font>


This space is designed for you to engage with your classmates about the material covered in Classwork 4.

Whether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.

If you have any specific questions for Byeong-Hak (**@bcdanl**) regarding the Classwork 4 materials or need clarification on any points, don't hesitate to ask here. 

All comments will be stored [here](https://github.com/bcdanl/210/discussions/).

Let's collaborate and learn from each other!


