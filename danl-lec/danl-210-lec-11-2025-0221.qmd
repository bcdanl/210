---
title: Lecture 11
subtitle: "`pandas` Basics - Missing Values; Duplicates; Reshaping"
format:
  clean-revealjs:
    self-contained: false
    chalkboard: true
    incremental: true
    code-annotations: hover
    scrollable: false

    # logo: logo-title-slide.png
author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2025-02-21
execute: 
  eval: true
  echo: true
callout-icon: false

from: markdown+emoji
include-after-body: target-hover.html # effect.html

# bibliography: refs.bib
---


```{r setup}
#| include: false
library(tidyverse)
library(skimr)
library(ggthemes)
library(hrbrthemes)


theme_set(theme_fivethirtyeight()+
          theme(strip.background =element_rect(fill="lightgray"),
                axis.title.x = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(10,0,0,0)),
                axis.title.y = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(0,10,0,0)),
                axis.text.x = element_text(size = rel(1.5)),
                axis.text.y = element_text(size = rel(1.5)),
                strip.text = element_text(size = rel(1.5)),
                legend.position = "top",
                legend.text = element_text(size = rel(1.5)),
                legend.title = element_text(size = rel(1.5))
                )
          )
```




# **Dealing with Missing Values** {background-color="#1c4982"}


## Dealing with Missing Values
:::{.nonincremental}
- Let's read `employment.csv` as `emp`.

```{.python}
import pandas as pd
# Below is for an interactive display of DataFrame in Colab
from google.colab import data_table
data_table.enable_dataframe_formatter()

emp = pd.read_csv("https://bcdanl.github.io/data/employment.csv")
```


:::



## Dealing with Missing Values

- Pandas often marks (1) missing text values and (2) missing numeric values with a `NaN` (not a number); 
  - It also marks missing datetime values with a `NaT` (not a time).

<!-- - We can use several pandas methods to isolate observations with either missing or non-missing values in a given variable. -->



## Dealing with Missing Values: The `isna()` and `notna()` methods

```{.python}
emp["Team"].isna()
emp["Start Date"].isna()
```

- The `isna()` method returns a Boolean `Series` in which `True` denotes that an observation's value is missing.
  - Is a value of a variable "XYZ" missing?


## Dealing with Missing Values: The `isna()` and `notna()` methods

```{.python}
# Below two are equivalent.
emp["Team"].notna()
~emp["Team"].isna()
```

- The `notna()` method returns the inverse `Series`, one in which `True` indicates that an observation's value is present.

- We use the tilde symbol (`~`) to invert a Boolean `Series`.

- **Q**. How can we pull out employees with non-missing `Team` values?




## Dealing with Missing Values: The `value_counts(dropna = False)` method

```{.python}
emp["Mgmt"].isna().sum()
emp["Mgmt"].value_counts()
emp["Mgmt"].value_counts(dropna = False)
```

- One way to missing data counts is to use the `isna().sum()` on a `Series`.
  - `True` is 1 and `False` is 0.

- Another way to get missing data counts is to use the `.value_counts()` method on a `Series`. 
  - If we use the `dropna = False` option, we can also get a missing value count.





## Dealing with Missing Values: The `dropna()` method

```{.python}
emp.dropna()
```

- The `dropna()` method removes observations that hold any `NaN` or `NaT` values.



## Dealing with Missing Values: The `dropna()` method with `how`

```{.python}
emp.dropna(how = "all")
```
- We can pass the `how` parameter an argument of `"all"` to remove observations in which all values are missing.

- Note that the `how` parameter’s default argument is `"any"`. 



## Dealing with Missing Values: The `dropna()` method with `subset`

```{.python}
emp.dropna(subset = ["Gender"])
```

- We can use the `subset` parameter to target observations with a missing value in a specific variable.
  - The above example removes observations that have a missing value in the `Gender` variable.




## Dealing with Missing Values: The `dropna()` method with `subset`

```{.python}
emp.dropna(subset = ["Start Date", "Salary"])
```

- We can also pass the `subset` parameter a list of variables.




<!-- ## Dealing with Missing Values: The `dropna()` method with `thresh` -->

<!-- ```{.python} -->
<!-- emp.dropna(thresh = 4) -->
<!-- ``` -->

<!-- - The `thresh` parameter specifies a minimum threshold of non-missing values that an observation must have for pandas to keep it. -->




# **Dealing with Duplicates** {background-color="#1c4982"}

## Dealing with Duplicates with the `duplicated()` method

::: {.nonincremental}
- Missing values are a common occurrence in messy data sets, and so are duplicate values.

:::

```{.python}
emp["Team"].duplicated()
```

- The `duplicated()` method returns a Boolean `Series` that identifies duplicates in a variable.


## Dealing with Duplicates with the `duplicated()` method

```{.python}
emp["Team"].duplicated(keep = "first")
emp["Team"].duplicated(keep = "last")
~emp["Team"].duplicated()
```

- The `duplicated()` method’s `keep` parameter informs pandas which duplicate occurrence to keep. 
  - Its default argument, `"first"`, keeps the first occurrence of each duplicate value.
  - Its argument, `"last"`, keeps the last occurrence of each duplicate value.

<!-- - **Q**. How can we keep observations with the first occurrence of a value in the **Team** variable? -->





## Dealing with Duplicates with the `drop_duplicates()` method

```{.python}
emp.drop_duplicates()
```


- The `drop_duplicates()` method removes observations in which all values are equal to those in a previously encountered observations.




## Dealing with Duplicates with the `drop_duplicates()` method


::: {.nonincremental}
- Below is an example of the `drop_duplicates()` method:

:::

```{.python}
# Sample DataFrame with duplicate observations
data = {
    'Name': ['John', 'Anna', 'John', 'Mike', 'Anna'],
    'Age': [28, 23, 28, 32, 23],
    'City': ['New York', 'Paris', 'New York', 'London', 'Paris']
}

# pd.DataFrame( Series, List, or Dict ) creates a DataFrame
df = pd.DataFrame(data)  
df_unique = df.drop_duplicates()
```




## Dealing with Duplicates with the `drop_duplicates()` method

```{.python}
emp.drop_duplicates(subset = ["Team"])
```

- We can pass the `drop_duplicates()` method a `subset` parameter with a list of columns that pandas should use to determine an observation's uniqueness.

  - The above example finds the first occurrence of each unique value in the Team variable.




## Dealing with Duplicates with the `drop_duplicates()` method

```{.python}
emp.drop_duplicates(subset = ["Gender", "Team"])
```

- The above example uses a combination of values across the `Gender` and `Team` variables to identify duplicates.


## Dealing with Duplicates with the `drop_duplicates()` method

```{.python}
emp.drop_duplicates(subset = ["Team"], keep = "last")
emp.drop_duplicates(subset = ["Team"], keep = False)
```

- The `drop_duplicates()` method also accepts a `keep` parameter. 
  - We can pass it an argument of `"last"` to keep the observations with each duplicate value’s last occurrence.
  - We can pass it an argument of `False` to exclude all observations with duplicate values.
  
- **Q**. What does `emp.drop_duplicates(subset = ["First Name"], keep = False)` do?

- **Q**. Find a subset of all employees with a First Name of "Douglas" and a Gender of "Male". Then check which "Douglas" is in the DataFrame `emp.drop_duplicates(subset = ["Gender", "Team"])`.



## Pandas Basics

Let's do Questions 7-8 in [**Classwork 6**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-06.html)!



# **Reshaping `DataFrames`** {background-color="#1c4982"}

## Reshaping `DataFrames`
### Tidy `DataFrames`


<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/tidy-1.png" width="800px">
</p>

- There are three interrelated rules that make a `DataFrame` tidy:
  - Each *variable* is a *column*; each *column* is a *variable*.
  - Each *observation* is a *row*; each *row* is an *observation*.
  - Each *value* is a *cell*; each *cell* is a single *value*.




## Reshaping `DataFrames`


<!-- ```{.python} -->
<!-- import pandas as pd -->
<!-- # Below is for an interactive display of DataFrame in Colab -->
<!-- from google.colab import data_table -->
<!-- data_table.enable_dataframe_formatter() -->
<!-- ``` -->


- A `DataFrame` can be given in a format unsuited for the analysis that we would like to perform on it.
  - A `DataFrame` may have larger structural problems that extend beyond the data.
  - Perhaps the `DataFrame` stores its values in a format that makes it easy to extract a single row but difficult to aggregate the data.
  

- *Reshaping* a `DataFrame` means manipulating it into a different shape.

- In this section, we will discuss pandas techniques for molding a `DataFrame` into the shape we desire.



## Long vs. Wide `DataFrames`

:::{.nonincremental}
- The following `DataFrames` measure temperatures in two cities over two days.

:::

```{.python}
import pandas as pd
from google.colab import data_table
data_table.enable_dataframe_formatter()

df_wide = pd.DataFrame({
    'Weekday': ['Tuesday', 'Wednesday'],
    'Miami': [80, 83],
    'Rochester': [57, 62],
    'St. Louis': [71, 75]
})

df_long = pd.DataFrame({
    'Weekday': ['Tuesday', 'Wednesday', 'Tuesday', 'Wednesday', 'Tuesday', 'Wednesday'],
    'City': ['Miami', 'Miami', 'Rochester', 'Rochester', 'St. Louis', 'St. Louis'],
    'Temperature': [80, 83, 57, 62, 71, 75]
})
```


## Long vs. Wide `DataFrames`
- A `DataFrame` can store its values in **wide** or **long** format.
- These names reflect the direction in which the data set expands as we add more values to it.
  - A long `DataFrame` increases in height.
  - A wide `DataFrame` increases in width. 



## Long vs. Wide `DataFrames`
- The optimal storage format for a `DataFrame` depends on the insight we are trying to glean from it.
  - We consider making `DataFrames` **longer** if one *variable* is spread across multiple *columns*.
  - We consider making `DataFrames` **wider** if one *observation* is spread across multiple *rows*.



## Reshaping `DataFrames` 
### `melt()` and `pivot()`

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/pandas-melt-pivot.gif" width="400px">
</p>

- `melt()` makes `DataFrame` longer.
- `pivot()` and `pivot_table()` make `DataFrame` wider.



## Make `DataFrame` Longer with `melt()`

```{.python}
df_wide_to_long = (
    df_wide
    .melt()
)
```


## Make `DataFrame` Longer with `melt()`

```{.python}
df_wide_to_long = (
    df_wide
    .melt(id_vars = "Weekday")
)



```

:::{.nonincremental}
- `melt()` can take a few parameters:
  - `id_vars` is a container (`string`, `list`, `tuple`, or `array`) that represents the variables that will remain as is.
  - `id_vars` can indicate which column should be the “identifier”.


:::



## Make `DataFrame` Longer with `melt()`

```{.python}
df_wide_to_long = (
    df_wide
    .melt(id_vars = "Weekday",
          var_name = "City",
          value_name = "Temperature")
)

```

:::{.nonincremental}

- `melt()` can take a few parameters:
  - `var_name` is a `string` for the *name* of the variable whose values are taken from *column names* in a given wide-form DataFrame.
  - `value_name` is a `string` for the *name* of the variable whose values are taken from the *values* in a given wide-form DataFrame.

:::


<!-- ## Make `DataFrame` Longer with `melt()` -->

<!-- ```{.python} -->
<!-- df_wide_to_long = ( -->
<!--     df_wide -->
<!--     .melt(id_vars = "Weekday", -->
<!--           var_name = "City", -->
<!--           value_name = "Temperature", -->
<!--           value_vars = ['Miami', 'Rochester']) -->
<!-- ) -->
<!-- ``` -->

<!-- :::{.nonincremental} -->

<!-- - `melt()` can take a few parameters: -->
<!--   - `value_vars` parameter allows us to select which specific columns we want to “melt”. -->
<!--   - By default, it will melt all the columns not specified in the `id_vars` parameter. -->

<!-- ::: -->


## Make `DataFrame` Wider with `pivot()`
```{.python}
df_long_to_wide = (
    df_long
    .pivot(index = "Weekday",
           columns = "City",
           values = "Temperature"  
        )
    .reset_index()
    )
```

- When using `pivot()`, we need to specify a few parameters:
  - `index` that takes the *column* to pivot on;
  - `columns` that takes the *column* to be used to make the *new variable names* of the wider `DataFrame`;
  - `values` that takes the *column* that provides the *values* of the *variables* in the wider `DataFrame`.



## Reshaping `DataFrames`
:::{.nonincremental}
- Let's consider the following wide-form `DataFrame`, `df`, containing information about the number of courses each student took from each department in each year.

:::
```{.python}
dict_data = {"Name": ["Donna", "Donna", "Mike", "Mike"],
             "Department": ["ECON", "DANL", "ECON", "DANL"],
             "2018": [1, 2, 3, 1],
             "2019": [2, 3, 4, 2],
             "2020": [5, 1, 2, 2]}
df = pd.DataFrame(dict_data)

df_longer = df.melt(id_vars=["Name", "Department"], 
                    var_name="Year", 
                    value_name="Number of Courses")
```

- The `pivot()` method can also take a `list` of variable names for the `index` parameter.


## Reshaping `DataFrames`
:::{.nonincremental}
- Let's consider the following wide-form `DataFrame`, `df`, containing information about the number of courses each student took from each department in each year.

:::
```{.python}
dict_data = {"Name": ["Donna", "Donna", "Mike", "Mike"],
             "Department": ["ECON", "DANL", "ECON", "DANL"],
             "2022": [1, 2, 3, 1],
             "2023": [2, 3, 4, 2],
             "2024": [5, 1, 2, 2]}
df = pd.DataFrame(dict_data)

df_longer = df.melt(id_vars=["Name", "Department"], 
                    var_name="Year", 
                    value_name="Number of Courses")
```

**Q**. How can we use the `df_longer` to create the wide-form `DataFrame`, `df_wider`, which is equivalent to the `df`?





## Reshaping `DataFrames`

Let's do Part 1 of [**Classwork 7**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-07.html)!



# **Joining `DataFrames`** {background-color="#1c4982"}


## Joining `DataFrames`
### Relational Data

- Sometimes, one data set is scattered across multiple files.
  - The size of the files can be huge.
  - The data collection process can be scattered across time and space.
  - E.g., `DataFrame` for county-level data and `DataFrame` for geographic information, such as longitude and latitude.


- Sometimes we want to combine two or more `DataFrames` based on common data values in those `DataFrames`.
  -  This task is known in the database world as performing a "join."
  -  We can do this with the `merge()` method in Pandas.





## Joining `DataFrames`
### Relational Data

:::{.nonincremental}
-  The variables that are used to connect each pair of tables are called **keys**.

:::
```{r, out.width='100%', fig.align='center'}
#| eval: true
#| echo: false
text_tbl <- data.frame(
  Pandas = c("left", "right", "outer", "inner"),
  SQL = c("left outer",
"right outer", 
"full outer",
"inner"),
  Description = c("Keep all the observations in the left",  
                  "Keep all the observations in the right", 
                  "Keep all the observations in both left and right",
                  "Keep only the observations whose key values exist in both")
  )



# Create a DT datatable without search box and 'Show entries' dropdown
DT::datatable(text_tbl, rownames = FALSE,
              options = list(
  dom = 't', # This sets the DOM layout without the search box ('f') and 'Show entries' dropdown ('l')
  paging = FALSE, # Disable pagination
  columnDefs = list(list(
    targets = "_all", # Applies to all columns
    orderable = FALSE # Disables sorting
  ))
), callback = htmlwidgets::JS("
  // Change header background and text color
  $('thead th').css('background-color', '#1c4982');
  $('thead th').css('color', 'white');

  // Loop through each row and alternate background color
  $('tbody tr').each(function(index) {
    if (index % 2 == 0) {
      $(this).css('background-color', '#d1dae6'); // Light color for even rows
    } else {
      $(this).css('background-color', '#9fb2cb'); // Dark color for odd rows
    }
  });

  // Set text color for all rows
  $('tbody tr').css('color', 'black');

  // Add hover effect
  $('tbody tr').hover(
    function() {
      $(this).css('background-color', '#607fa7'); // Color when mouse hovers over a row
    }, 
    function() {
      var index = $(this).index();
      if (index % 2 == 0) {
        $(this).css('background-color', '#d1dae6'); // Restore even row color
      } else {
        $(this).css('background-color', '#9fb2cb'); // Restore odd row color
      }
    }
  );
")
)


```




## Joining `DataFrames` with `merge()`

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-setup.png" width="300">
</p>


:::: {.columns}

::: {.column width="50%"}
```{.python}
x = pd.DataFrame({
    'key': [1, 2, 3],
    'val_x': ['x1', 'x2', 'x3']
})

```

:::

::: {.column width="50%"}

```{.python}
y = pd.DataFrame({
    'key': [1, 2, 4],
    'val_y': ['y1', 'y2', 'y3']
})
```
:::

::::

- The colored column represents the "key" variable.
- The grey column represents the "value" column.



## Joining `DataFrames` with `merge()`
### Inner Join
- An **inner join** matches pairs of observations whenever their keys are equal:

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-inner.png" width="425">
</p>

```{.python}
# the default value for 'how' is 'inner'
# so it doesn't actually need to be specified
merge_inner = pd.merge(x, y, on='key', how='inner')
merge_inner_x = x.merge(y, on='key', how='inner')
merge_inner_x_how = x.merge(y, on='key')
```



## Joining `DataFrames` with `merge()`
### Left Join

- A **left join** keeps all observations in `x`.

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-left.png" width="425">
</p>

```{.python}
merge_left = pd.merge(x, y, on='key', how='left')
merge_left_x = x.merge(y, on='key', how='left')
```

- The most commonly used join is the left join.


## Joining `DataFrames` with `merge()`
### Right Join

- A **right join** keeps all observations in `y`.

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-right.png" width="425">
</p>


```{.python}
merge_right = pd.merge(x, y, on='key', how='right')
merge_right_x = x.merge(y, on='key', how='right')
```



## Joining `DataFrames` with `merge()`
### Outer (Full) Join

- A **full join** keeps all observations in `x` and `y`.

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-full.png" width="412.5">
</p>


```{.python}
merge_outer = pd.merge(x, y, on='key', how='outer')
merge_outer_x = x.merge(y, on='key', how='outer')
```




## Joining `DataFrames` with `merge()`
### Duplicate keys: one-to-many

- One `DataFrame` has duplicate keys (a one-to-many relationship). 

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-one-to-many.png" width="375">
</p>


:::: {.columns}

::: {.column width="47.5%"}
```{.python}
x = pd.DataFrame({
    'key':[1, 2, 2, 3],
    'val_x':['x1', 'x2', 'x3', 'x4']})
```

:::

::: {.column width="52.5%"}
```{.python}
y = pd.DataFrame({
    'key':[1, 2],
    'val_y':['y1', 'y2'] })
one_to_many = x.merge(y, on='key', 
                         how='left')
```

:::

::::


## Joining `DataFrames` with `merge()`
### Duplicate keys: many-to-many
- Both `DataFrames` have duplicate keys (many-to-many relationship).

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/join-many-to-many.png" width="350">
</p>



:::: {.columns}

::: {.column width="45%"}
```{.python}
x = pd.DataFrame({
  'key':[1, 2, 2, 3],
  'val_x':['x1','x2','x3','x4']})

```

:::


::: {.column width="55%"}
```{.python}
y = pd.DataFrame({
  'key': [1, 2, 2, 3],
  'val_y': ['y1', 'y2', 'y3', 'y4'] })
many_to_many = x.merge(y, on='key', 
                          how='left')

```
:::

::::



## Joining `DataFrames` with `merge()`
### Defining the key columns

:::{.nonincremental}
- If the left and right columns do not have the same name for the key variables, we can use the `left_on` and `right_on` parameters instead.

:::

:::: {.columns}

::: {.column width="50%"}
```{.python}
x = pd.DataFrame({
  'key_x': [1, 2, 3],
  'val_x': ['x1', 'x2', 'x3']
})
```
:::

::: {.column width="50%"}

```{.python}
y = pd.DataFrame({
  'key_y': [1, 2],
  'val_y': ['y1', 'y2'] })

keys_xy = 
  x.merge(y, left_on = 'key_x', 
             right_on = 'key_y', 
             how = 'left')
```
:::
::::



## Joining `DataFrames` with `merge()`


Let's do Part 2 of [**Classwork 7**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-07.html)!




# **Data Concatenation** {background-color="#1c4982"}


## Data Concatenation
:::{.nonincremental}
- Concatenation can be thought of as appending a row or column to our data. 
  - This approach is possible if our data was split into parts or if we performed a calculation that we want to append to our existing data set.

- Let's consider the following example DataFrames:

```{.python}
df1 = pd.read_csv('https://bcdanl.github.io/data/concat_1.csv')
df2 = pd.read_csv('https://bcdanl.github.io/data/concat_2.csv')
df3 = pd.read_csv('https://bcdanl.github.io/data/concat_3.csv')
```


- We will be working with `.index` and `.columns` in this Section.
```{.python}
df1.index
df1.columns
```

:::

## Data Concatenation
###  Add Rows  

:::{.nonincremental}

- Concatenating the `DataFrames` on top of each other uses the `concat()` method. 
  - All of the `DataFrames` to be concatenated are passed in a `list`.
  
```{.python}
row_concat = pd.concat([df1, df2, df3])
row_concat
```

:::

## Data Concatenation
###  Add Rows  
:::{.nonincremental}

- Let's consider a new Series and concatenate it with `df1`:
```{.python}
# create a new row of data
new_row_series = pd.Series(['n1', 'n2', 'n3', 'n4'])
new_row_series


# attempt to add the new row to a dataframe
df = pd.concat([df1, new_row_series])
df
```

-  Not only did our code not append the values as a row, but it also created a new column completely misaligned with everything else.
  - Why?

:::

## Data Concatenation
###  Add Rows  

:::{.nonincremental}

- To fix the problem, we need turn our `Series` into a `DataFrame`.
  - This data frame contains one row of data, and the column names are the ones the data will bind to.

```{.python}
new_row_df = pd.DataFrame(
  # note the double brackets to create a "row" of data
  data =[["n1", "n2", "n3", "n4"]],
  columns =["A", "B", "C", "D"],
)

df = pd.concat([df1, new_row_df])
```

:::

## Data Concatenation
###   Add Columns  

:::{.nonincremental}

- Concatenating *columns* is very similar to concatenating *rows*. 
  - The main difference is the `axis` parameter in the `concat()` method. 
  - The default value of `axis` is `0` (or `axis = "index"`), so it will concatenate data in a row-wise fashion. 
  - If we pass `axis = 1` (or `axis = "columns"`) to the function, it will concatenate data in a column-wise manner.
  
  
```{.python}
col_concat = pd.concat([df1, df2, df3], axis = "columns")
```

:::

## Data Concatenation
###   Add Columns  
:::{.nonincremental}

- We can use `ignore_index=True` to reset the column indices, so that we do not have duplicated column names.

```{.python}
pd.concat([df1, df2, df3], axis="columns", ignore_index=True)
```

:::


## Concatenate with Different Indices  
:::{.nonincremental}

- What would happen when the row and column indices are not aligned?

- Let’s modify our DataFrames for the next few examples.
```{.python}
# rename the columns of our dataframes
df1.columns = ['A', 'B', 'C', 'D']
df2.columns = ['E', 'F', 'G', 'H']
df3.columns = ['A', 'C', 'F', 'H']
```

-  If we try to concatenate these DataFrames as we did, the DataFrames now do much more than simply stack one on top of the other.
```{.python}
row_concat = pd.concat([df1, df2, df3])
```

:::


## Concatenate with Different Indices  

:::{.nonincremental}

- We can set `join = 'inner'` to keep only the columns that are shared among the data sets.
```{.python}
pd.concat([df1, df2, df3], join ='inner')
```


- If we use the `DataFrames` that have columns in common, only the columns that all of them share will be returned.
```{.python}
pd.concat([df1, df3], join ='inner',  ignore_index =False)
```

:::


## Concatenate with Different Indices  

:::{.nonincremental}

- Let’s modify our DataFrames further.
```{.python}
# re-indexing the rows of our DataFrames
df1.index = [0, 1, 2, 3]
df2.index = [4, 5, 6, 7]
df3.index = [0, 2, 5, 7]
```

:::


## Concatenate with Different Indices  

:::{.nonincremental}

- When we concatenate along `axis="columns"` `(axis=1`), the new DataFrames will be added in a column-wise fashion and matched against their respective row indices.

```{.python}
col_concat = pd.concat([df1, df2, df3], axis="columns")
```


- Just as we did when we concatenated in a row-wise manner, we can choose to keep the results only when there are matching indices by using `join="inner"`.
```{.python}
pd.concat([df1, df3], axis ="columns", join='inner')
```

:::



## Data Concatenation

Let's do Part 3 of [**Classwork 7**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-07.html)!




