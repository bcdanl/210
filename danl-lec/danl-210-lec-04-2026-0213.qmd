---
title: Lecture 4
subtitle: "Data Collection II: Web-scrapping Primer; Scrapping Data with `selenium`"
format:
  clean-revealjs:
    self-contained: false
    chalkboard: true
    incremental: true
    code-annotations: hover
    scrollable: false

    # logo: logo-title-slide.png
author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2026-02-13

execute: 
  eval: true
  echo: true
callout-icon: false

from: markdown+emoji
include-after-body: target-hover.html # effect.html

# bibliography: refs.bib
---


```{r setup}
#| include: false
library(tidyverse)
library(skimr)
library(ggthemes)
library(hrbrthemes)


theme_set(theme_fivethirtyeight()+
          theme(strip.background =element_rect(fill="lightgray"),
                axis.title.x = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(10,0,0,0)),
                axis.title.y = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(0,10,0,0)),
                axis.text.x = element_text(size = rel(1.5)),
                axis.text.y = element_text(size = rel(1.5)),
                strip.text = element_text(size = rel(1.5)),
                legend.position = "top",
                legend.text = element_text(size = rel(1.5)),
                legend.title = element_text(size = rel(1.5))
                )
          )
```


# **Premier on Web-scrapping** {background-color="#1c4982"}

## Data Collection via Web-scraping

- Web pages can be a rich data source, but **web scraping is powerful**.
  - Careless scraping can **harm websites, violate rules, or compromise privacy**.
  
- Our goal in this module:
  - Learn the **web fundamentals** (client/server, HTTPS, URL, HTML/DOM),
  - Understand **ethical, responsible scraping**


## “Legal” Is Not the Same as “Ethical”

<div class="fragment" style="font-size:1.75rem; text-align:center; margin-top:1.5rem;">
  <blockquote>
    *"If you can see things in your web browser, you can scrape them."*
  </blockquote>
</div>

- *Legally (U.S.)*: **publicly available** data may sometimes be scraped using automated tools in US (e.g., [**hiQ Labs vs. LinkedIn Corp.**](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn))
- *But legality ≠ permission or responsibility*:
  - *Technically*: it may be possible.
  - *Ethically*: you still must consider terms or service (ToS), robots.txt, privacy, and data minimization.
  - *Practically*: you can trigger blocks or harm service quality (e.g., overloading servers, ToS/privacy issues).

:::: fragment
::: {.callout-warning}

**Legal ≠ ethical.** Even if data is “public,” ToS, privacy expectations, and platform blocks still matter.

:::
::::



# Web Basics: Clients and Servers {background-color="#1c4982"}

## Clients and Servers


<div style="text-align: center; width: 75%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/client-server.jpg" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

- Devices on the web act as **clients** and **servers**.
- Your browser is a **client**; websites and data live on **servers**.
  - **Client**: your computer/phone + a browser (Chrome/Firefox/Safari).
  - **Server**: a computer that stores webpages/data and sends them when requested.
- When you load a page, your browser sends a **request** and the server sends back a **response** (the page content).


## Hypertext Transfer Protocol Secure (HTTPS)

::: nonincremental
- **HTTP** is how clients and servers communicate.
- **HTTPS** is encrypted HTTP (safer).

:::

When we type a URL starting with `https://`:

1. Browser finds the server.
2. Browser and server establish a secure connection.
3. Browser sends a request for content.
4. Server responds (e.g., **200 OK**) and sends data.
5. Browser decrypts and displays the page.



## HTTP Status Codes

```{.python}
# library for making HTTPS requests in Python
import requests
```

:::: {.columns}
::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/210'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **200 OK** → success; content returned.
:::

::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/2100'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **404 Not Found** → URL/page doesn’t exist (typo, removed page, broken link).
:::
::::



## URL (what you’re actually requesting)

```{r, echo=FALSE, eval=TRUE, out.width='75%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/URL.png")
```

- A **URL** is a location for a resource on the internet.
- Often includes:
  - protocol (`https`)
  - domain (`example.com`)
  - path (`/products`)
  - **query string** (`?id=...&cat=...`) ← common in data pages
  - fragment (`#section`) ← in-page reference



# HTML Basics {background-color="#1c4982"}

## The Big Idea: Scraping = Selecting from HTML

- When you “scrape,” you usually:
  1) load a page,
  2) read the **HTML**,
  3) extract specific elements (title, price, table, links, etc.).
- **If you don’t understand HTML/DOM, you can’t reliably target the right data.**
- Selenium is not “magic”—it automates a browser, but you still need:
  - **DOM inspection**
  - **selectors** (XPath)
  - stable element identifiers (`id`, `class`, attributes)



## HTML in one sentence

- **HTML** is the markup that defines the **structure** of a web page (headings, paragraphs, links, tables, etc.).



## HTML Example

:::{.nonincremental}
- A minimal HTML document:
:::

```{.html}
<!DOCTYPE html>
<html>
  <head>
    <title>Page Title</title>
  </head>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
  </body>
</html>
```



## HTML elements (what you actually scrape)

- Most HTML is built from **elements** like:

```{.html}
<tagname>Content goes here...</tagname>
```

- Common ones you’ll extract:
  - headings: `<h1> ... </h1>`
  - text blocks: `<p> ... </p>`
  - links: `<a href="..."> ... </a>`
  - tables: `<table> ... </table>`



## HTML body: links and images

#### `<a>` (link)

```{.html}
<a href="https://www.w3schools.com">This is a link</a>
```

- The `href` attribute is often what you scrape.

#### `<img>` (image)

```{.html}
<img src="w3schools.jpg" alt="W3Schools.com" width="104" height="142">
```

- You may scrape `src` (image URL) or `alt` (description).



## HTML Tables (common data source)

- Table structure:
  - `<table>` table container
  - `<tr>` row
  - `<th>` header cell
  - `<td>` data cell

```{.html}
<table style="width:100%">
  <tr>
    <th>Firstname</th>
    <th>Lastname</th> 
    <th>Age</th>
  </tr>
  <tr>
    <td>Eve</td>
    <td>Jackson</td>
    <td>94</td>
  </tr>
</table>
```



## Lists you’ll see in the wild

### Unordered list (`<ul>`)

```{.html}
<ul>
  <li>Coffee</li>
  <li>Tea</li>
  <li>Milk</li>
</ul>
```

### Ordered list (`<ol>`)

```{.html}
<ol>
  <li>Coffee</li>
  <li>Tea</li>
  <li>Milk</li>
</ol>
```



## Containers you’ll target a lot: `<div>` and `<span>`

### `<div>` – block-level container

```{.html}
<div style="background-color:black;color:white;padding:20px;">
  <h2>London</h2>
  <p>London is the capital city of England...</p>
</div>
```

- Often used to group major page sections.

### `<span>` – inline container

```{.html}
<p>My mother has <span style="color:blue;font-weight:bold">blue</span> eyes...</p>
```

- Often used inside text; can carry classes/ids that help selection.



## Inspecting HTML (your #1 Selenium skill)

- Open DevTools:
  - **F12** (Chrome/Firefox), or right-click → **Inspect**
- Use it to find:
  - element text,
  - `id` / `class`,
  - attributes (like `href`, `data-*`),
  - whether content is static or injected by JavaScript.



## Document Object Model (DOM)
### The Browser’s "Tree" of the Page


:::: {.columns}
::: {.column width="50%"}

<div style="text-align: center; width: 100%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/DOM.png" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

:::

::: {.column width="50%"}
- The browser represents HTML as the **DOM** (Document Object Model).
- Selenium interacts with the **DOM**.
- Scraping often becomes:
  - “Find the node”
  - “Extract its text/attribute”
  
:::

::::






## HTML in Browser vs. HTML in DevTools

:::: {.columns}
::: {.column width="50%"}
```{r, echo=FALSE, eval=TRUE, out.width='75%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/html-code.png")
```
:::
::: {.column width="50%"}
```{r, echo=FALSE, eval=TRUE, out.width='50%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/html-web.png")
```
:::
::::

