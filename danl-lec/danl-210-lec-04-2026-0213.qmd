---
title: Lecture 4
subtitle: "Data Collection II: Web-scrapping Primer; Scrapping Data with `selenium`"
format:
  clean-revealjs:
    self-contained: false
    chalkboard: true
    incremental: true
    code-annotations: hover
    scrollable: false

    # logo: logo-title-slide.png
author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2026-02-13

execute: 
  eval: true
  echo: true
callout-icon: false

from: markdown+emoji
include-after-body: target-hover.html # effect.html

# bibliography: refs.bib
---


```{r setup}
#| include: false
library(tidyverse)
library(skimr)
library(ggthemes)
library(hrbrthemes)


theme_set(theme_fivethirtyeight()+
          theme(strip.background =element_rect(fill="lightgray"),
                axis.title.x = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(10,0,0,0)),
                axis.title.y = 
                  element_text(angle = 0,
                               size = rel(1.5),
                               margin = margin(0,10,0,0)),
                axis.text.x = element_text(size = rel(1.5)),
                axis.text.y = element_text(size = rel(1.5)),
                strip.text = element_text(size = rel(1.5)),
                legend.position = "top",
                legend.text = element_text(size = rel(1.5)),
                legend.title = element_text(size = rel(1.5))
                )
          )
```


# üï∏  **Premier on Web-scrapping** {background-color="#1c4982"}

## üé£üì• Data Collection via Web-scraping

- Web pages can be a rich data source, but **web scraping is powerful**.
  - Careless scraping can **harm websites, violate rules, or compromise privacy**.
  
- Our goal in this module:
  - Learn the **web fundamentals** (client/server, HTTPS, URL, HTML/DOM),
  - Understand **ethical, responsible scraping**


## ‚öñÔ∏èü§î ‚ÄúLegal‚Äù Is Not the Same as ‚ÄúEthical‚Äù

<div class="fragment" style="font-size:1.75rem; text-align:center; margin-top:1.5rem;">
  <blockquote>
    *"If you can see things in your web browser, you can scrape them."*
  </blockquote>
</div>

- *Legally (U.S.)*: **publicly available** data may sometimes be scraped using automated tools in US (e.g., [**hiQ Labs vs. LinkedIn Corp.**](https://en.wikipedia.org/wiki/HiQ_Labs_v._LinkedIn))
- *But legality ‚â† permission or responsibility*:
  - *Technically*: it may be possible.
  - *Ethically*: you still must consider terms or service (ToS), robots.txt, privacy, and data minimization.
  - *Practically*: you can trigger blocks or harm service quality (e.g., overloading servers, ToS/privacy issues).

:::: fragment
::: {.callout-warning}

**Legal ‚â† ethical.** Even if data is ‚Äúpublic,‚Äù ToS, privacy expectations, and platform blocks still matter.

:::
::::



# üåê **Web Basics: Clients and Servers** {background-color="#1c4982"}

## üíª‚ÜîÔ∏èüóÑÔ∏èÔ∏è Clients and Servers

<div style="text-align: center; width: 75%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/client-server.jpg" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

- Devices on the web act as **clients** and **servers**.
- Your browser is a **client**; websites and data live on **servers**.
  - **Client**: your computer/phone + a browser (Chrome/Firefox/Safari).
  - **Server**: a computer that stores webpages/data and sends them when requested.
- When you load a page, your browser sends a **request** and the server sends back a **response** (the page content).


## üîíüõ°Ô∏è Hypertext Transfer Protocol Secure (HTTPS)

::: nonincremental
- **HTTP** is how clients and servers communicate.
- **HTTPS** is encrypted HTTP (safer).

:::

When we type a URL starting with `https://`:

1. Browser finds the server.
2. Browser and server establish a secure connection.
3. Browser sends a request for content.
4. Server responds (e.g., **200 OK**) and sends data.
5. Browser decrypts and displays the page.



## üö¶üî¢ HTTP Status Codes

```{.python}
# library for making HTTPS requests in Python
import requests
```

<div style="display:block; margin:25px;"></div>

:::: {.columns}
::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/210'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **200 OK** ‚Üí success; content returned.
:::

::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/2100'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **404 Not Found** ‚Üí URL/page doesn‚Äôt exist (typo, removed page, broken link).
:::
::::



## üîóüìç URL (what you‚Äôre actually requesting)

```{r, echo=FALSE, eval=TRUE, out.width='75%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/URL.png")
```

- A **URL** is a location for a resource on the internet.
- Often includes:
  - Protocol (`https`)
  - Domain (`example.com`)
  - Path (`/products`)
  - **Query string** (`?id=...&cat=...`) ‚Üê common in data pages
  - Fragment (`#section`) ‚Üê in-page reference



# üèóÔ∏è **HTML Basics** {background-color="#1c4982"}

## üéØü§è The Big Idea: Scraping = Selecting from HTML

- **HTML** (HyperText Markup Language) is the markup that defines the **structure** of a web page (headings, paragraphs, links, tables, etc.).

- When you ‚Äúscrape,‚Äù you usually:
  1. Load a page
  2. Examine the **HTML** 
  3. Extract specific elements (title, price, table, links, etc.)
- **If you don‚Äôt understand HTML, you can‚Äôt reliably target the right data.**
- Selenium is not ‚Äúmagic‚Äù‚Äîit automates a browser, but you still need to:
  - Inspect the HTML to identify and target the right elements




<!-- ## HTML Example -->

<!-- :::{.nonincremental} -->
<!-- - A minimal HTML document: -->
<!-- ::: -->

<!-- ```{.html} -->
<!-- <!DOCTYPE html> -->
<!-- <html> -->
<!--   <head> -->
<!--     <title>Page Title</title> -->
<!--   </head> -->
<!--   <body> -->
<!--     <h1>My First Heading</h1> -->
<!--     <p>My first paragraph.</p> -->
<!--   </body> -->
<!-- </html> -->
<!-- ``` -->




## üñºÔ∏èüÜöüìù HTML in Browser vs. HTML Source Code

:::: {.columns}
::: {.column width="50%"}

<div style="text-align: center; width: 100%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/html-web-210.png" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>


:::
::: {.column width="50%"}


<div style="text-align: center; width: 100%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/html-code-210.png" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

:::
::::




## üå≥üìë Document Object Model (DOM)
### The Browser‚Äôs "Tree" of the Page

:::: {.columns}
::: {.column width="50%"}

<div style="text-align: center; width: 100%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/DOM-model.png" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

:::

::: {.column width="50%"}
- The browser represents HTML as the **DOM** (Document Object Model).
- Selenium interacts with the **DOM**.
- Scraping often becomes:
  - ‚ÄúFind the node‚Äù
  - ‚ÄúExtract its text/attribute‚Äù
  
:::

::::



## üîçüïµÔ∏è Inspecting HTML (your #1 web-scrapping skill)

- Open a **Chrome** browser.
- Open DevTools:
  - **F12**, or right-click ‚Üí **Inspect**
- Use it to find:
  - Element text
  - `id` / `class`
  - Attributes (like `href`, `data-*`)



## üß±üß© HTML Elements (what you actually scrape)

::: nonincremental
- Most HTML is built from **elements** like:

:::

```{.html}
<tagname>Content goes here...</tagname>
```

- Common ones you‚Äôll extract:
  - Headings: `<h1> ... </h1>`
  - Text blocks: `<p> ... </p>`
  - Links: `<a href="..."> ... </a>`
  - Tables: `<table> ... </table>`
  - Containers: `<div> ... </div>`
  - Inline text: `<span> ... </span>`


## üîóüñºÔ∏è HTML Body: Links and Images

:::: nonincremental

::: fragment
#### `<a>` (Link)

```{.html}
<a href="https://www.w3schools.com">This is a link</a>
```

- The `href` attribute is often what you scrape.

:::

::: fragment
#### `<img>` (Image)

```{.html}
<img src="w3schools.jpg" alt="W3Schools.com" width="104" height="142">
```

- You may scrape `src` (image URL) or `alt` (description).

:::
::::


## üßæ HTML Tables

:::: {.columns}
::: {.column width="50%"}
```{.html}
<table style="width:100%">
  <tr>
    <th>Firstname</th>
    <th>Lastname</th> 
    <th>Age</th>
  </tr>
  <tr>
    <td>Eve</td>
    <td>Jackson</td>
    <td>94</td>
  </tr>
</table>
```

:::
::: {.column width="50%"}


- Table structure:
  - `<table>` table container
  - `<tr>` row
  - `<th>` header cell
  - `<td>` data cell

:::
::::

## üìã Lists you‚Äôll see in the wild



:::: nonincremental

:::: {.columns}

::: {.column width="50%"}
::: fragment

#### ‚ö´ Unordered List (`<ul>`)

```{.html}
<ul>
  <li>Coffee</li>
  <li>Tea</li>
  <li>Milk</li>
</ul>
```

:::

<div style="display:block; margin:25px;"></div>

::: fragment

- Coffee
- Tea
- Milk

:::

:::


::: {.column width="50%"}
::: fragment
#### üî¢ Ordered List (`<ol>`)

```{.html}
<ol>
  <li>Coffee</li>
  <li>Tea</li>
  <li>Milk</li>
</ol>
```

:::

<div style="display:block; margin:25px;"></div>

::: fragment

1. Coffee
2. Tea
3. Milk

:::

:::

::::
:::: 

## üéØüì¶ Containers you‚Äôll target a lot: `<div>` and `<span>`


:::: nonincremental

::: fragment
### `<div>` ‚Äì block-level container

```{.html}
<div style="background-color:black;color:white;padding:20px;">
  <h2>Seoul</h2>
  <p>Seoul is the capital city of South Korea...</p>
</div>
```

:::

::: fragment

<div style="background-color:black;color:white;padding:20px;">
  <h2>Seoul</h2>
  <p>Seoul is the capital city of South Korea...</p>
</div>

:::

::: fragment

- Often used to group major page sections.

:::



::: fragment
### `<span>` ‚Äì inline container

```{.html}
<p>My mother has <span style="color:blue;font-weight:bold">blue</span> eyes...</p>
```
:::

<div style="display:block; margin:-10px;"></div>

::: fragment

<p>My mother has <span style="color:blue;font-weight:bold">blue</span> eyes...</p>

:::

<div style="display:block; margin:-10px;"></div>

::: fragment

- Often used inside text.

:::
::::



# **Web-scrapping with Python `selenium`** {background-color="#1c4982"}

## What is Selenium?

<div style="display:block; margin:-15px;"></div>

<div style="text-align: center; width: 33%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/se-py.jpg" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

- **Selenium** is a tool that lets Python **control a real web browser** (like Chrome or Firefox) automatically.
- It is used for:
  - **Web automation** (click buttons, fill forms, scroll pages)
  - **Web scraping** when a website is **dynamic** (JavaScript loads content after the page opens)
- Selenium works by interacting with the page‚Äôs **DOM** (Document Object Model):
  - It finds elements in HTML
  - Then reads **text/attributes** or performs actions (click, type, scroll)


## WebDriver
- **WebDriver** is an wire protocol that defines a language-neutral interface for controlling the behavior of web browsers. 

- The purpose of WebDriver is to **control the behavior of web browsers programmatically**, allowing automated interactions such as:
	-	Extracting webpage content
	-	Opening a webpage
	-	Clicking buttons
	-	Filling out forms
	-	Running automated tests on web applications


- **Selenium WebDriver** refers to both the language bindings and the implementations of browser-controlling code.


## Driver

- Each browser requires a specific **WebDriver** implementation, called a **driver**.
	-	Web browsers (e.g., Chrome, Firefox, Edge) do not natively understand Selenium WebDriver commands.
	-	To bridge this gap, each browser has its own **WebDriver implementation**, known as a **driver**.

- The **driver** handles communication between Selenium WebDriver and the browser. 
  -	This **driver** acts as a middleman between **Selenium WebDriver** and the actual browser.

- Different browsers have specific drivers:
  - **ChromeDriver** for Chrome
  - **GeckoDriver** for Firefox

<!-- - Newer versions of Python `selenium` do **not** require manual driver downloads. -->

## Interaction Diagram

:::{.nonincremental}

- A simplified diagram of how **WebDriver** interacts with **browser** might look like this:

:::

<div style="display:block; margin:-50px;"></div>

<p align="center">
  <img src="https://bcdanl.github.io/lec_figs/driver.png" width="400px">
</p>


- WebDriver interacts with the browser via the **driver** in a two-way communication process:
  1. **Sends commands** (e.g., open a page, click a button) to the browser.
  2. **Receives responses** from the browser.



## Setting up

- Install the Chrome or FireFox web-browser if you do not have either of them.
  - I will use the Chrome.
  
- Install Selenium using `pip`:
  -  On the Spyder Console, run the following: 
    - `pip install selenium`

- [**Selenium with Python**](https://selenium-python.readthedocs.io/index.html) is a well-documented reference.


## Setting up - `webdriver.Chrome()`

:::{.nonincremental}

- To begin with, we import (1) `webdriver` from `selenium` and (2) the `By` and `Options` classes.
  - `webdriver.Chrome()` opens the Chrome browser that is being controlled by automated test software, `selenium`.

:::

```{.python}
# Import the necessary modules from the Selenium library
from selenium import webdriver  # Main module to control the browser
from selenium.webdriver.common.by import By  # Helps locate elements on the webpage
from selenium.webdriver.chrome.options import Options  # Allows setting browser options

# Create an instance of Chrome options
options = Options()
options.add_argument("window-size=1400,1200")  # Set the browser window size to 1400x1200

# Initialize the Chrome WebDriver with the specified options
driver = webdriver.Chrome(options=options)  # Correct implementation

# Now you can use 'driver' to control the Chrome browser
```




## `get()` Method in WebDriver

:::{.nonincremental}

-	`get(url)` from `webdriver` opens the specified URL in a web browser.
-	When using `webdriver` in Google Chrome, you may see the message:
	- _‚ÄúChrome is being controlled by automated test software.‚Äù_

:::

```{.python}
form_url = "https://qavbox.github.io/demo/webtable/"
driver.[?](form_url)
driver.close()
driver.quit()
```


-	`close()` terminates the current browser window.
-	`quit()` completely exits the `webdriver` session, closing a browser window.


## **Inspecting Web Elements** with `find_element()`

- Once the Google Chrome window loads with the provided URL, we need to **find specific elements** to interact with.
  - The easiest way to identify elements is by using **Developer Tools** to inspect the webpage structure.

- To inspect an element:
  1. **Right-click** anywhere on the webpage.
  2. **Select** the **Inspect** option from the pop-up menu.
  3. In the `Elements` panel, **hover over** the DOM structure to locate the desired element.


## **Inspecting Web Elements** with `find_element()`

- When inspecting an element, look for:
  - **HTML tag** (e.g., `<input>`, `<button>`, `<div>`) used for the element.
  - **Attributes** (e.g., `id`, `class`, `name`) that define the element.
  - **Attribute values** that help uniquely identify the element.
  - **Page structure** to understand how elements are nested within each other.




<!-- - Place the cursor anywhere on the webpage, *right-click* to open a pop-up menu, then *select* the **Inspect** option. -->
<!--   - In the `Elements` window, move the cursor over the DOM structure of the page until it reaches the desired element.  -->
<!--   - We then need to find information such as what HTML tag is used for the *element*, the defined *attribute*, and the *values* for the attributes and the structure of the page. -->


# **Locating Web Elements by `find_element()` & `find_elements()`** {background-color="#1c4982"}


## Locating Web Elements by `find_element()`

:::{.nonincremental}
- There are various strategies to locate elements in a page.
  
:::

```{.html}
find_element(By.ID, "id")
find_element(By.CLASS_NAME, "class name")
find_element(By.NAME, "name")
find_element(By.CSS_SELECTOR, "css selector")
find_element(By.TAG_NAME, "tag name")
find_element(By.LINK_TEXT, "link text")
find_element(By.PARTIAL_LINK_TEXT, "partial link text")
find_element(By.XPATH, "xpath")
```


- Selenium provides the `find_element()` method to locate elements in a page.

- To find multiple elements (these methods will return a **list**):
  - `find_elements()`


## **`find_element(By.ID, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1) 

- `find_element(By.ID, "")` & `find_elements(By.ID, "")`:
  - Return element(s) that match a given **ID** attribute value.

- Example HTML code where an element has an ID attribute `form1`:

```{.html}
<form id="form1">...</form>
```

## (2) 

- Example of locating the form using `find_element(By.ID, "")`:

```{.python}
form = driver.find_element(By.ID, "form1")
form.text  # Retrieves text content if available
```
:::
::::




## **`find_element(By.CLASS_NAME, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- `find_element(By.CLASS_NAME, "")` & `find_elements(By.CLASS_NAME, "")`:
  - Return element(s) matching a specific **class attribute**.

- Example HTML code with a `homebtn` class:

```{.html}
<div class="homebtn" align="center">...</div>
```

## (2)

```{.python}
home_button = driver.find_element(By.CLASS_NAME, "homebtn")
home_button.click()  # Clicks the home button
driver.back()  # Navigates back to the previous page
```
:::
::::

## **`find_element(By.NAME, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- `find_element(By.NAME, "")` & `find_elements(By.NAME, "")`:
  - Return element(s) with a matching **name attribute**.

- Example HTML code with a name attribute `home`:

```{.html}
<input type="button" class="btn" name="home" value="Home" />
```

## (2)

```{.python}
home_button2 = driver.find_element(By.NAME, "home")
home_button2.click()
driver.back()
```
:::
::::

## **`find_element(By.CSS_SELECTOR, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- `find_element(By.CSS_SELECTOR, "")` & `find_elements(By.CSS_SELECTOR, "")`:
  - Locate element(s) using a **CSS selector**.

- Inspect the webpage using browser Developer Tools.
- Locate the desired element in the Elements panel.
- Right-click and select **Copy selector**
  - Let's find out CSS selector for the Home button.

## (2)

```{.python}
home_button3 = driver.find_element(By.CSS_SELECTOR, "body > div > a > input")
home_button3.click()
driver.back()
```
:::
::::


## **`find_element(By.TAG_NAME, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- `find_element(By.TAG_NAME, "")` & `find_elements(By.TAG_NAME, "")`:
  - Locate element(s) by a specific **HTML tag**.

## (2)

```{.python}
table01 = driver.find_element(By.ID, "table01")
thead = table01.find_element(By.TAG_NAME, "thead")
thead.text
```
:::
::::

## **`find_element(By.LINK_TEXT, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- `find_element(By.LINK_TEXT, "")` & `find_elements(By.LINK_TEXT, "")`:
  - Locate link(s) using the exact **text displayed**.

- Example HTML for a Selenium link:

```{.html}
<a href="http://www.selenium.dev/">Selenium</a>
```

## (2)

```{.python}
selenium_link = driver.find_element(By.LINK_TEXT, "Selenium")
selenium_link.click()
driver.back()
```
:::
::::

## **`find_element(By.PARTIAL_LINK_TEXT, "")`**

:::: {.nonincremental}
::: {.panel-tabset}
## (1)

- Finds link(s) containing **partial** text.

## (2)

```{.python}
Selen_links = driver.find_elements(By.PARTIAL_LINK_TEXT, "qav")
print(len(Selen_links))
Selen_links[0].click()
driver.back()
```
:::
::::



## **`find_element(By.XPATH, "")`**
### Understanding XPath

- `find_element(By.XPATH, "")` & `find_elements(By.XPATH, "")`:
  - Return element(s) that match the specified XPath query.

- **XPath** is a query language for searching and locating nodes in an **XML document**.
  - Supported by all major web browsers.
  - Used in Selenium to find elements when **ID, name, or class attributes** are not available.
  - Powerful for navigating complex HTML structures.

## Basic XPath Syntax

```{.html}
//tag_name[@attribute='value']
```

- `//` ‚Üí Selects elements anywhere in the document.
- `tag_name` ‚Üí HTML tag (`input`, `div`, `span`, etc.).
- `@attribute` ‚Üí Attribute name (`id`, `class`, `aria-label`, etc.).
- `value` ‚Üí Attribute's assigned value.



## Absolute vs. Relative XPath

- **Absolute XPath** ‚Üí Defines the full path from the root node.
  - Reliable if the webpage structure does not change.
- **Relative XPath** ‚Üí Defines the path starting from a known element.
  - More flexible‚Äîdoesn‚Äôt break as easily if the structure changes.


## Example: Finding a Table Element with XPath

- XPath can use attributes **other than ID, name, or class** to locate elements.
- Suppose we want to retrieve data from a **second table** on a webpage.
- The table contains multiple `<tr>` (rows) and `<th>` (headers) **without** an easily identifiable ID or class.
- `find_element(By.TAG_NAME, "")` is **not reliable** due to multiple `<tr>` and `<th>` tags.

## Extracting XPath from Developer Tools

:::: {.nonincremental}

- **Inspect** the webpage using browser Developer Tools.
- Locate the desired element in the **Elements** panel.
- **Right-click** and select **Copy XPath**.
- Example extracted XPath:

:::

```{.html}
//*[@id="table02"]/tbody/tr[1]/td[1]
/html/body/form/fieldset/div/div/table/tbody/tr[1]/td[1]
```

## Finding an Element Using XPath
:::{.nonincremental}
- Locate **"Tiger Nixon"** in the second table:

:::

```{.python}
elt = driver.find_element(By.XPATH, '//*[@id="table02"]/tbody/tr[1]/td[1]')
print(elt.text)  # Output the extracted text
```

## When to Use XPath

- **Use XPath when:**
  - The element lacks a unique **ID** or **class**.
  - Other locator methods (`By.ID`, `By.CLASS_NAME`, etc.) **don't work**.
- **Limitations:**
  - XPath is **less efficient** than ID-based locators.
  - Page structure changes may break XPath-based automation.

- **For our tasks, however, XPath remains a reliable and effective method!**



## **Retrieving Attribute Values with `get_attribute()`**


:::: {.nonincremental}
::: {.panel-tabset}
## (1) 

- `get_attribute()` extracts an element's **attribute value**.
- Useful for retrieving **hidden** properties not visible on the page.

```{.html}
<a href="https://www.selenium.dev/">Selenium</a>
<input id="btn" class="btn" type="button" onclick="change_text(this)" value="Delete">
```


## (2)

```{.python}
driver.find_element(By.XPATH, '//*[@id="table01"]/tbody/tr[2]/td[3]/a').get_attribute('href')
driver.find_element(By.XPATH, '//*[@id="btn"]').get_attribute('value')
```

:::
::::

## Web-scrapping with Python `selenium`

Let's do [**Classwork 4**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-04.html)!



# **`NoSuchElementException` and `WebDriverWait`** {background-color="#1c4982"}


## `NoSuchElementException` and `try-except` blocks

```{.python}
from selenium.common.exceptions import NoSuchElementException
try:
    elem = driver.find_element(By.XPATH, "element_xpath")
    elem.click()
except NoSuchElementException:
    pass
```


- When a web element is not found, it throws the `NoSuchElementException`.
  - `try-except` can be used to avoid the termination of the selenium code.

- This solution is to address the **inconsistency** in the DOM among the seemingly same pages.


## Explicit wait with `time.sleep()`

```{.python}
import time

# example webpage
url = "https://qavbox.github.io/demo/delay/"
driver.get(url)

driver.find_element(By.XPATH, '//*[@id="one"]/input').click()
time.sleep(5)
element = driver.find_element(By.XPATH, '//*[@id="two"]')
element.text
```


-  The `time.sleep()` method is an explicit wait to set the condition to be an exact time period to wait.

- In general, a more efficient solution than `time.sleep()` would be to make `WebDriver()` wait only as long as required. 





## Implicit wait with `implicitly_wait()`

```{.python}
driver.find_element(By.XPATH, '//*[@id="oneMore"]/input[1]').click()
driver.implicitly_wait(10)  # Wait up to 10 seconds for elements to appear
element2 = driver.find_element(By.ID, 'delay')
element2.text
```


- Implicit wait with `implicitly_wait()` directs the `webdriver` to wait for a certain measure of time before throwing an exception. 
  - Applies globally for the lifetime of the driver session.

- Once this time is set, `webdriver` will wait for the element before the exception occurs.





## Explicit wait with  `WebDriverWait` and `EC`

:::: {.nonincremental}
::: {.panel-tabset}
## (0)

- An explicit wait allows you to **wait for a specific condition** before continuing.
- Uses:
  - Wait for elements to **appear**, **be visible**, or **be clickable**.
  - More flexible and precise than implicit waits.

```{.python}
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
```

## (1)

- To wait for `presence_of_element_located`:

```{.python}
element = ( 
  WebDriverWait(driver, 20)  # 20 is timeout in seconds when an expectation is called
  .until(
    EC.presence_of_element_located(
      (By.XPATH, "element_xpath")
      )
    )
) 
```


## (2)
- To wait for `visibility_of_element_located`:

```{.python}
element = (
  WebDriverWait(driver, 20)
  .until(
    EC.visibility_of_element_located(
      (By.CSS_SELECTOR, "element_css")
      )
  )
)
```





## (3)
- To wait for `element_to_be_clickable`:

```{.python}
element = (
  WebDriverWait(driver, 20)
  .until(
    EC.element_to_be_clickable(
      (By.LINK_TEXT, "element_link_text")
      )
    )
)
```



::: 
::::

# **Selenium with `pd.read_html()` for Table Scrapping** {background-color="#1c4982"}

## Selenium with `pd.read_html()` for Table Scrapping
- Yahoo! Finance has probably renewed its database system, so that `yfinance` does not work now.

- [Yahoo! Finance](https://finance.yahoo.com/quote/NVDA/history/?p=NVDA&period1=1672531200&period2=1743379200) uses web table to display historical data about a company's stock.

- Let's use Selenium with `pd.read_html()` to collect stock price data!


## Selenium with `pd.read_html()` for Yahoo! Finance Data

::: {.panel-tabset}
## Setup
```{.python}
import pandas as pd
import os, time, random
from selenium import webdriver
from selenium.webdriver.common.by import By
from io import StringIO

# Set working directory
os.chdir('/Users/bchoe/.../lecture-code/')

# Launch Chrome browser
driver = webdriver.Chrome()

```

- `StringIO` turns that string into a file-like object, which is what `pd.read_json()` expects moving forward.


## URL

```{.python}
# Load content page
url = 'https://finance.yahoo.com/quote/MSFT/history/?p=MSFT&period1=1672531200&period2=1743379200'
driver.get(url)
time.sleep(random.uniform(3, 5))  # wait for table to load
```

- `period1` and `period2` values for Yahoo Finance URLs uses **Unix timestamps** (number of seconds since January 1, 1970), 
  - 1672531200 ‚Üí 2023-01-01
  - 1743379200 ‚Üí 2025-03-31

## `get_attribute("outerHTML")`

```{.python}
# Extract the <table> HTML element
table_html = driver.find_element(By.TAG_NAME, 'table').get_attribute("outerHTML")

# Parse the HTML table into a pandas DataFrame
df = pd.read_html(StringIO(table_html))[0]
```

- `.get_attribute("outerHTML")`: gets the entire HTML from the WebElement.

- `pd.read_html()` parses HTML tables from a given URL or from raw HTML content, and returns a list of DataFrames.

:::

## Web-scrapping with Python `selenium`

Let's do [**Classwork 5**](https://bcdanl.github.io/210/danl-cw/danl-210-cw-05.html)!



