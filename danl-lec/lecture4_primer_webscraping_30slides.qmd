---
title: Lecture 4
subtitle: "Primer on Web-scraping (Python + Selenium)"
format:
  clean-revealjs:
    self-contained: false
    chalkboard: true
    incremental: true
    code-annotations: hover
    scrollable: false

author:
  - name: Byeong-Hak Choe
    email: bchoe@geneseo.edu
    affiliations: SUNY Geneseo
date: 2026-02-13

execute: 
  eval: true
  echo: true
callout-icon: false

from: markdown+emoji
include-after-body: target-hover.html
---

```{r setup}
#| include: false
library(tidyverse)
library(skimr)
library(ggthemes)
library(hrbrthemes)

theme_set(
  theme_fivethirtyeight()+
    theme(
      strip.background =element_rect(fill="lightgray"),
      axis.title.x = element_text(angle = 0, size = rel(1.5), margin = margin(10,0,0,0)),
      axis.title.y = element_text(angle = 0, size = rel(1.5), margin = margin(0,10,0,0)),
      axis.text.x  = element_text(size = rel(1.5)),
      axis.text.y  = element_text(size = rel(1.5)),
      strip.text   = element_text(size = rel(1.5)),
      legend.position = "top",
      legend.text  = element_text(size = rel(1.5)),
      legend.title = element_text(size = rel(1.5))
    )
)
```

# **Primer on Web-scraping** {background-color="#1c4982"}

## Data Collection via Web-scraping

- Web pages can be a rich data source, but **web scraping is powerful**.
  - Careless scraping can **harm websites, violate rules, or compromise privacy**.
- Our goal in this module:
  - Learn the **web fundamentals** (client/server, HTTPS, URL, HTML/DOM),
  - Practice **ethical, responsible scraping** (ToS/robots.txt, privacy, rate limiting),
  - Use **Selenium** when pages are **dynamic** (JS-driven).

## Learning goals (today)

By the end, you should be able to:

- Explain what scraping is‚Äîand when it‚Äôs appropriate.
- Describe **client/server** and what happens in an **HTTPS request/response**.
- Explain **HTML + DOM** and why they matter for Selenium.
- Tell whether a page is **static vs dynamic** (JS-driven).
- Apply a simple **responsible scraping checklist**.

# Ethics First: Scraping is Not ‚ÄúFree‚Äù {background-color="#1c4982"}

## The quote (and the correction)

> *"If you can see things in your web browser, you can scrape them."*

- *Technically*: often true.
- *Ethically*: incomplete.
- *Practically*: risky.
  - You can unintentionally overload servers.
  - You can violate terms, copyright, or privacy expectations.

## Ethical considerations (what we owe others)

- Respect **Terms of Service (ToS)**.
- Respect **robots.txt** (not law, but a strong norm for automated access).
- Respect **copyright / intellectual property**.
- Respect **server resources** (rate limits, delays, off-peak runs).
- Respect **privacy**:
  - Avoid personal data unless necessary and justified.
  - Minimize what you collect.
  - Store it securely.

## robots.txt in one slide

- `robots.txt` is a website‚Äôs **public ‚Äúrules-of-the-road‚Äù** for crawlers.
- It typically specifies:
  - which bots the rules apply to (`User-agent`)
  - which paths are blocked (`Disallow`) or allowed (`Allow`)
- **Key point:** robots.txt is a **request** for polite behavior, not a password wall.

::: {.callout-note}
**Class norm:** Always check `robots.txt` before scraping and follow it unless you have explicit permission to do otherwise.
:::

## Responsible scraping checklist ‚úÖ

Before you scrape, ask:

1) **Permission?** (ToS, robots.txt, course policy)
2) **API available?** Prefer official APIs/datasets when they exist.
3) **Sensitive data?** If yes: rethink, minimize, or stop.
4) **Gentle execution?** Add delays, avoid parallel hammering, cache results.
5) **Reproducible?** Record source, timestamp, query logic, and versions.

::: {.callout-warning}
**Legal ‚â† ethical.** Even if data is ‚Äúpublic,‚Äù ToS, privacy expectations, and platform blocks still matter.
:::

## Polite scraping behavior (practical habits)

- Add **randomized delays** between requests/actions.
- Keep volumes small; start with **samples**.
- Avoid scraping behind logins/paywalls unless you have explicit permission.
- Don‚Äôt bypass anti-bot protections.
- Stop if you‚Äôre blocked or suspect you‚Äôre causing load.

# Web Basics: Clients and Servers {background-color="#1c4982"}

## Clients and servers

<div style="text-align: center; width: 75%; margin: auto;">
  <img src="https://bcdanl.github.io/lec_figs/client-server.jpg" style="width: 100%; margin-bottom: -50px;">
  <p style="font-weight: bold;"></p>
</div>

- Devices on the web act as **clients** and **servers**.
- Your browser is a **client**; websites and data live on **servers**.
  - **Client**: your computer/phone + a browser (Chrome/Firefox/Safari).
  - **Server**: a computer that stores webpages/data and sends them when requested.
- When you load a page, your browser sends a **request** and the server sends back a **response**.

## HTTPS request/response (what your browser does)

::: nonincremental
- **HTTP** is how clients and servers communicate.
- **HTTPS** is encrypted HTTP (safer).
:::

When you type a URL starting with `https://`:

1. Browser finds the server.
2. Browser and server establish a secure connection.
3. Browser sends a request for content.
4. Server responds (e.g., **200 OK**) and sends data.
5. Browser decrypts and displays the page.

## HTTP Status Codes (a fast check)

```{.python}
import requests
```

:::: {.columns}
::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/210'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **200 OK** ‚Üí success; content returned.
:::

::: {.column width="50%"}
```{.python}
p = 'https://bcdanl.github.io/2100'
response = requests.get(p)
print(response.status_code)
print(response.reason)
```

- **404 Not Found** ‚Üí URL/page doesn‚Äôt exist (typo, removed page, broken link).
:::
::::

## URL anatomy (what you‚Äôre actually requesting)

```{r, echo=FALSE, eval=TRUE, out.width='75%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/URL.png")
```

- A **URL** is a location for a resource on the internet.
- Often includes:
  - protocol (`https`)
  - domain (`example.com`)
  - path (`/products`)
  - **query string** (`?id=...&cat=...`) ‚Üê common in data pages
  - fragment (`#section`) ‚Üê in-page reference

# HTML Basics {background-color="#1c4982"}

## The big idea: scraping = selecting from HTML

- Typical workflow:
  1) load a page
  2) read the **HTML**
  3) extract specific elements (title, price, table rows, links, etc.)
- **If you don‚Äôt understand HTML/DOM, you can‚Äôt reliably target the right data.**
- Selenium is not ‚Äúmagic‚Äù‚Äîit automates a browser, but you still need:
  - **DOM inspection**
  - **selectors** (CSS selectors / XPath)
  - stable identifiers (`id`, `class`, attributes)

## HTML in one sentence + tiny example

- **HTML** defines the **structure** of a web page.

```{.html}
<!DOCTYPE html>
<html>
  <head>
    <title>Page Title</title>
  </head>
  <body>
    <h1>My First Heading</h1>
    <p>My first paragraph.</p>
  </body>
</html>
```

## Common tags you scrape: links and images

#### `<a>` (link)

```{.html}
<a href="https://www.w3schools.com">This is a link</a>
```

- `href` is often the data you want.

#### `<img>` (image)

```{.html}
<img src="w3schools.jpg" alt="W3Schools.com" width="104" height="142">
```

- `src` (image URL) or `alt` can be useful.

## HTML tables (very common data source)

- Table structure:
  - `<table>` container
  - `<tr>` row
  - `<th>` header cell
  - `<td>` data cell

```{.html}
<table>
  <tr><th>Firstname</th><th>Lastname</th><th>Age</th></tr>
  <tr><td>Eve</td><td>Jackson</td><td>94</td></tr>
</table>
```

## Lists + containers you‚Äôll see everywhere

### Lists
```{.html}
<ul><li>Coffee</li><li>Tea</li></ul>
<ol><li>Coffee</li><li>Tea</li></ol>
```

### Containers
```{.html}
<div> ...block-level container... </div>
<span> ...inline container... </span>
```

- `<div>` groups sections; `<span>` targets small inline pieces.

## Inspecting HTML (your #1 Selenium skill)

- Open DevTools:
  - **F12** (Chrome/Firefox), or right-click ‚Üí **Inspect**
- Use it to find:
  - element text,
  - `id` / `class`,
  - attributes (`href`, `data-*`),
  - whether content is static or injected by JavaScript.

## DOM: the browser‚Äôs ‚Äútree‚Äù of the page

- The browser represents HTML as the **DOM** (Document Object Model).
- Selenium interacts with the **DOM**, not with pixels.
- Scraping often becomes:
  - ‚Äúfind the node‚Äù
  - ‚Äúextract its text/attribute‚Äù

```{r, echo=FALSE, eval=TRUE, out.width='40%', fig.align='center'}
knitr::include_graphics("https://bcdanl.github.io/lec_figs/DOM.png")
```

## Selectors preview: CSS vs XPath (reliability tip)

- **CSS selectors** (common, concise): `#id`, `.class`, `tag`, `[attr=value]`
- **XPath** (powerful, flexible): good when CSS isn‚Äôt enough
- Reliability rule:
  - Prefer **stable** selectors (unique IDs, meaningful classes, attributes),
  - Avoid fragile ‚Äúdeep‚Äù paths that break when layout changes.

# Static vs. Dynamic Pages {background-color="#1c4982"}

## JavaScript (JS) makes pages dynamic

- JS can:
  - load more data after the page ‚Äúfinishes‚Äù loading,
  - update content when you click/scroll,
  - build HTML dynamically.

**Key test:**  
If data appears only after a click/scroll or delay, the page is likely **JS-driven**.

## Tool choice: requests/BS4 vs Selenium

- If a page is static:
  - prefer `requests + BeautifulSoup` (faster, simpler, more ‚Äúpolite‚Äù)
- Use Selenium when:
  - content is generated by JavaScript,
  - you must click/scroll to reveal content,
  - the workflow requires real browser interaction (with permission).

# Selenium-ready preview {background-color="#1c4982"}

## Three things you must know next (ethics + reliability)

## 1) CSS selectors = *target the right element* üéØ

- Selenium selects **DOM elements** using selectors.
- Reliability tip:
  - Prefer stable identifiers: `#id`, unique classes, meaningful attributes.
  - Avoid fragile selectors that break when layout changes.

::: {.callout-note}
**Ethics angle:** Accurate targeting helps you collect **only what you need** (data minimization).
:::

## 2) Explicit waits = *don‚Äôt fight the website* ‚è≥

- Modern pages load content **asynchronously** (JavaScript).
- Use **explicit waits** instead of guessing with `time.sleep()`.

::: {.callout-note}
**Ethics angle:** Proper waits reduce extra reloads/retries ‚Üí fewer unnecessary requests.
:::

## 3) Rate limiting = *be polite to the server* üßë‚Äçü§ù‚Äçüßë

- Add pauses between actions/requests (keep volumes small).
- Prefer randomized delays and caching.

::: {.callout-warning}
**Ethics angle:** Rate limiting respects shared infrastructure‚Äîdon‚Äôt degrade service for real users.
:::

## Guardrails for class projects

- Start with **small samples** and **few pages**.
- Log what you did (URL, time, pages visited, outputs saved).
- Stop immediately if:
  - you‚Äôre blocked,
  - you suspect you‚Äôre causing load,
  - you see personal/sensitive info you don‚Äôt need.

# Wrap-up {background-color="#1c4982"}

- **Ethics first:** ToS/robots.txt, privacy, rate limiting, reproducibility.
- **HTML/DOM** is the map‚Äîscraping is selecting elements from it.
- **Selenium** is for dynamic, JS-driven pages (heavier, easier to misuse).
- Next lecture: Selenium basics (driver, selectors, waits, extracting text/attributes, safe pacing).
