[
  {
    "objectID": "listing-danl-210-hw.html",
    "href": "listing-danl-210-hw.html",
    "title": "DANL 210 - Homework",
    "section": "",
    "text": "Title\nSubtitle\nDate\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "blog-listing.html",
    "href": "blog-listing.html",
    "title": "Big & tiny insights through data",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNBA\n\n3 min\n\n\nByeong-Hak Choe\n\n\nFebruary 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Basics\n\n1 min\n\n\nByeong-Hak Choe\n\n\nFebruary 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeer Markets\n\n5 min\n\n\nByeong-Hak Choe\n\n\nNovember 2, 2023\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n1 min\n\n\nByeong-Hak Choe\n\n\nOctober 27, 2023\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "danl-qa/danl-210-qa.html",
    "href": "danl-qa/danl-210-qa.html",
    "title": "DANL 210 - Discussion and Q & A Board",
    "section": "",
    "text": "Welcome to our Discussion and Q & A Board! üëã \nThis space is designed for you to engage with your classmates about the course materials.\nWhether you are looking to delve deeper into the slides, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions to Byeong-Hak (@bcdanl) or your classmates or need clarification on any points, don‚Äôt hesitate to ask here.\nLet‚Äôs collaborate and learn from each other!\nPlease note that all our comments are recorded in here, regardless of whether comments are displayed in this page or not.\n\n\n\n Back to top"
  },
  {
    "objectID": "listing-danl-210-lec.html",
    "href": "listing-danl-210-lec.html",
    "title": "DANL 210 - Lecture",
    "section": "",
    "text": "Title\nSubtitle\nDate\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "danl-cw/danl-210-cw-01.html",
    "href": "danl-cw/danl-210-cw-01.html",
    "title": "Classwork 1",
    "section": "",
    "text": "Getting a GitHub account\nStep 1. Create the GitHub account with your Geneseo email.\n\nGo to GitHub.\nClick ‚ÄúSign up for GitHub‚Äù.\n\n\nChoose your GitHub username carefully:\n\nhttps://USERNAME.github.io will be the address for your website.\nByeong-Hak‚Äôs GitHub username is bcdanl, so that Byeong-Hak owns the web address https://bcdanl.github.io.\n\nIt is recommended to have a username with all lower cases.\n\n\n\n\n\nInstalling git if you do not have one.\nStep 2.\n\nCheck whether git is installed in your laptop.\n\n\nFrom the Console Pane in RStudio, click Terminal tab.\n\n\n\n\n\nFrom the Terminal, run the following command to check if your laptop has git installed.\n\ngit --version\n\nIf your computer has git installed, you will see the message below and you do not need to install git:\n\ngit version 2.xx\n\nIf your computer does not have git installed, you will see the message below and you need to install git:\n\n'git' is not recognized as an internal or external command\n\n\nInstall git if you do not have one. Move to the next step if you have git installed in your laptop.\n\n\n\n\nMac\n\nGo to http://git-scm.com/downloads, and download the file.\nClick ‚ÄúmacOS‚Äù, scroll down the webpage, and then click ‚Äúinstaller‚Äù from the Binary installer section.\nRun the downloaded file.\n\n\n\n\nWindows\n\nGo to https://gitforwindows.org, and download the file.\nRun the downloaded file.\n\n\n\n\n\nKeep clicking ‚ÄúNext‚Äù to complete the installation of git.\nAfter the git installation is done, close RStudio and re-open it.\n\n\nHow to open git installation file on Mac?\n\nRun the downloaded file.\nClick Okay\nGo to ‚ÄúSetting‚Äù &gt; ‚ÄúPrivacy and Security‚Äù\nGo to ‚ÄúGeneral‚Äù or scroll down\nClick ‚ÄúOpen Anyway‚Äù\n\n\n\n\n\n\n\n\nSetting up GitHub Credential on your local Git.\nStep 3. In Terminal, run the following commands one by one:\ngit config --global user.email \"YOUR_EMAIL_ADDRESS\"\ngit config --global user.name \"YOUR_USERNAME\"\nFor example, the email address for my GitHub account is bchoe@geneseo.edu, and my GitHub username is bcdanl, so that I ran below:\ngit config --global user.email \"bchoe@geneseo.edu\"\ngit config --global user.name \"bcdanl\"\n\nStep 4. Obtain a personal access token (PAT) from GitHub.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"usethis\")\nusethis::create_github_token()\n\nThen, click ‚ÄúGenerate token‚Äù in the pop-upped web browser.\nWe can think of GitHub‚Äôs personal access token as a password that expires. You can decide how long it remains valid. My recommendation is to set its expiration for May 31, 2025, or later.\n\n\n\n\n\nThen, copy the generated PAT, and paste it to your clipboard or R script.\n\n\nStep 5. Set the GitHub credential using the PAT.\n\nIn RStudio Console, run the followings line by line:\n\ninstall.packages(\"gitcreds\")\ngitcreds::gitcreds_set()\n\nYou will be asked to provide your PAT.\nPaste your PAT to the RStudio Console, and then hit Enter.\n\n\n\n\n\n\n\nNote\n\n\n\n\nIt does not harm to create multiple PAT for one GitHub account.\nAfter the PAT expires, you should repeat the following if you want to update your GitHub website:\n\n\nCreate a new PAT:\n\nusethis::create_github_token()\n\nReplace the current PAT with the new PAT:\n\ngitcreds::gitcreds_set()\n\nSelect the option 2: Replace these credentials by typing 2 and hitting Enter on R Console.\n\n\n\n\n\n\nEstablishing the Connection between GitHub repo and your local Git\nStep 6. Login to your GitHib and make the repository.\n\nFrom https://github.com, click the plus [+] icon in the upper right corner and select ‚ÄúNew repository‚Äù.\nName this repo USERNAME.github.io, which will be the domain for your website.\n\n\ne.g., If your GitHub username is abc9, the name of your repo should be abc9.github.io, not abc_9.github.io.\n\n\nThen, copy the web address of your GitHub repo, https://github.com/USERNAME/USERNAME.github.io\n\n\nFor example, the web address for Byeong-Hak‚Äôs GitHub repo is https://github.com/bcdanl/bcdanl.github.io.\n\n\nStep 7. Create a RStudio project with Version Control\n\n\n\n\nClick ‚ÄúProject (None)‚Äù at the top-right corner in RStudio.\nClick ‚ÄúNew Project‚Äù &gt; ‚ÄúVersion Control‚Äù &gt; ‚ÄúGit‚Äù\nPaste the web address of your GitHub repo to the Repository URL menu.\nClick ‚ÄúBrowse‚Äù to select the parent directory for your local project directory (I recommend ‚ÄúDocuments‚Äù folder.)\nClick ‚ÄúCreate‚Äù\n\n\n\n\n\n\n\nNote\n\n\n\nIf Step 7 does not work on your laptop, try below Steps 7-1 and 7-2 instead. If Step 7 DOES work well, skip Steps 7-1 and 7-2.\n\n\nStep 7-1. Use git clone to establish the connection between GitHub repo and your local laptop:\n\nChange directory to ‚ÄúDocuments‚Äù in Terminal using cd command.\n\ncd &lt;pathname of \"Documents\" directory&gt;\n\nHere, you need to know the pathname of ‚ÄúDocuments‚Äù directory.\nFor example, LAPTOP_USERNAME below is not your GitHub username but one for your local laptop.\n\nMac\ncd /Users/LAPTOP_USERNAME/Documents\nWindows\ncd C:/Users/LAPTOP_USERNAME/Documents\n\nUse git clone to creates a local copy of the GitHub Repository.\n\ngit clone &lt;repository-url&gt;\n\nFor example,\n\ngit clone https://github.com/USERNAME/USERNAME.github.io\n\nStep 7-2. Create a RStudio project from Existing Directory\n\nClick ‚ÄúProject (None)‚Äù at the top-right corner in RStudio.\nClick ‚ÄúNew Project‚Äù &gt; ‚ÄúExisting Directory‚Äù\nClick ‚ÄúBrowse‚Äù to select the local copy of the GitHub Repository\nClick ‚ÄúCreate Project‚Äù\n\n\n\n\nDownloading Website Template Files\nStep 8. Download the files of website template:\n\nGo to the following webpage: https://github.com/bcdanl/danl-website-template\nFrom the webpage above, click the green icon &lt; &gt; Code, and then click ‚ÄúDownload Zip‚Äù\nExtract the Zip file you have downloaded\nIf there are the files, .gitignore, .DS_Store, or *.Rproj, in the folder, delete all of them.\nMove all the files that were compressed in the Zip file to your local project directory, USERNAME.github.io.\n\n\nSelect all the files in the danl-website-template folder (Tip: Ctrl + A (Windows) / command + A (Mac) selects all files in a directory).\nThen, Ctrl + C (Windows) / command + C (Mac) to copy them.\nThen, go to your local project directory USERNAME.github.io.\nThen, Ctrl + V (Windows) / command + V (Mac) to paste them to your local project directory USERNAME.github.io.\n\n\nRemove the danl-website-template directory from your local project directory, if you have one.\n\n\nAll the website files should be located at the same level with the R Project file (USERNAME.github.io.Rproj), shown below.\n\n\n\n\n\n\n\nPushing the Website Files to the GitHub repository\n\n\n\nStep 8. Push the files to your GitHub repository\n\nOn Terminal within RStudio, execute the following 3-step git commands, which will stage, commit, and push all the files in the local working directory to your GitHub repository:\n\n\ngit add . adds changes in your local working directory (e.g., edited files, new files, deleted files) to the staging area, which is a temporary area where you can prepare your next commit\n\ngit add .\n\ngit commit -m \"...\" records the changes in the staging area as a new snapshot in the local working directory, along with a message describing the changes.\n\ngit commit -m \"any message to describe the changes\"\n\ngit push uploads the local changes to the online repository in GitHub.\n\ngit push\n\nStep 9. Check whether the files are well uploaded.\n\nGo to the webpages of your GitHub repository and your website:\n\nhttps://github.com/USERNAME/USERNAME.github.io.git\nhttps://USERNAME.github.io\nRefresh the webpages (Ctrl + R for Windows users; cmd + R for Mac users)\n\nAdd a URL for your website (https://YOUR_GITHUB_USERNAME.github.io/) in About section in your GihtHub repository webpage by clicking the setting. Below describes how to do it:\n\n\n\n\nDiscussion\nWelcome to our Classwork 1 Discussion Board! üëã \nThis space is designed for you to engage with your classmates about the material covered in Classwork 1.\nWhether you are looking to delve deeper into the content, share insights, or have questions about the content, this is the perfect place for you.\nIf you have any specific questions for Byeong-Hak (@bcdanl) regarding the Classwork 1 materials or need clarification on any points, don‚Äôt hesitate to ask here.\nAll comments will be stored here.\nLet‚Äôs collaborate and learn from each other!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html",
    "href": "posts/beer-markets/beer-markets.html",
    "title": "Beer Markets",
    "section": "",
    "text": "Diving into the complex world of what people like in their beer, the beer_markets.csv dataset comes across as a goldmine of data, showing us the detailed interactions between buyers and their favorite beers. This dataset covers everything from how much and at what price people are buying beer to how deals and brand loyalty influence their decisions, across different types of people and places. As we start digging into this dataset, we aim to uncover the patterns that show what really influences the modern beer drinker‚Äôs choices, offering up valuable insights for marketers, industry watchers, and beer lovers. By breaking down the data, our exploration will shine a light on the factors that drive consumer behavior in the beer market, giving us a full picture of the trends that shape this lively industry.\nCode\n# Creating an interactive table\n!pip install itables\nfrom itables import init_notebook_mode\nfrom itables import show\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Reading the CSV file\nbeer_data = pd.read_csv(\"https://bcdanl.github.io/data/beer_markets.csv\")\nshow(beer_data)\n\n\n\n\n\n\n    \n      \n      hh\n      _purchase_desc\n      quantity\n      brand\n      dollar_spent\n      beer_floz\n      price_per_floz\n      container\n      promo\n      market\n      buyertype\n      income\n      childrenUnder6\n      children6to17\n      age\n      employment\n      degree\n      cow\n      race\n      microwave\n      dishwasher\n      tvcable\n      singlefamilyhome\n      npeople\n    \n  Loading... (need help?)\nCode\n# Setting up the visualization settings\nsns.set(style=\"whitegrid\")\n\n# Calculate total quantity and spending for each brand\nbrand_summary = beer_data.groupby('brand').agg({'quantity':'sum', 'dollar_spent':'sum'}).reset_index()\n\n# Sort by total quantity and spending\nbrand_summary_sorted_quantity = brand_summary.sort_values('quantity', ascending=False)\nbrand_summary_sorted_spent = brand_summary.sort_values('dollar_spent', ascending=False)\nCode\n# Plotting total quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=brand_summary_sorted_quantity, palette='viridis')\nplt.title('Total Quantity of Beer Purchased by Brand')\nplt.xlabel('Total Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe bar charts above display the total quantity of beer purchased and the total spending by brand. From the looks of it, certain brands dominate in terms of quantity sold and total spending, indicating their popularity.\nNow, let‚Äôs calculate the average quantity purchased and average spending per purchase. For this, we‚Äôll consider each row in the dataset as a separate purchase and compute the averages accordingly.\nCode\n# Calculate average quantity purchased and average spending per purchase\naverage_purchase = beer_data.groupby('brand').agg({\n    'quantity': 'mean',\n    'dollar_spent': 'mean'\n}).reset_index()\n\n# Sort by average quantity and average spending\naverage_purchase_sorted_quantity = average_purchase.sort_values('quantity', ascending=False)\naverage_purchase_sorted_spent = average_purchase.sort_values('dollar_spent', ascending=False)\n\n# Plotting average quantity for each brand\nplt.figure(figsize=(10, 8))\nsns.barplot(x='quantity', y='brand', data=average_purchase_sorted_quantity, palette='viridis')\nplt.title('Average Quantity of Beer Purchased by Brand')\nplt.xlabel('Average Quantity')\nplt.ylabel('Brand')\nplt.show()\nThe visualizations above depict the average quantity of beer purchased per brand and the average spending per brand. This shows which brands tend to be bought in larger quantities on average and which brands tend to have higher spending per purchase, which could be indicative of their price point or the purchase of premium products.\nNext, we‚Äôll look at the total spending across different markets to see if there are any notable differences in spending habits geographically. To do this, we‚Äôll sum up the spending in each market and visualize it.\nCode\n# Calculate total spending in each market\nmarket_spending_summary = beer_data.groupby('market').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nmarket_spending_summary_sorted = market_spending_summary.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending in each market\nplt.figure(figsize=(12, 18))\nsns.barplot(x='dollar_spent', y='market', data=market_spending_summary_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Market')\nplt.xlabel('Total Spending')\nplt.ylabel('Market')\nplt.show()\nThe bar chart illustrates the total spending on beer by market, showcasing the differences in spending habits across various regions. Some markets have significantly higher spending, which could be due to a variety of factors including market size, consumer preferences, or economic factors.\nNow, let‚Äôs move on to the second analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "href": "posts/beer-markets/beer-markets.html#demographic-analysis",
    "title": "Beer Markets",
    "section": "Demographic Analysis",
    "text": "Demographic Analysis\nWe will examine which demographics are buying what kind of beer and whether spending habits vary by demographics such as age, employment, and race. For this, we could look at:\n\nSpending by age group\nSpending by employment status\nSpending by race\n\nI‚Äôll start by analyzing spending by age group.\n\n\nCode\n# Calculate total spending by age group\nage_group_spending = beer_data.groupby('age').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nage_group_spending_sorted = age_group_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by age group\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='age', data=age_group_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Age Group')\nplt.xlabel('Total Spending')\nplt.ylabel('Age Group')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart demonstrates the total spending on beer segmented by age group, highlighting which age groups spend the most on beer. It appears that certain age groups are more dominant in beer spending, which may align with the purchasing power or preferences of those groups.\nNext, we will examine spending by employment status.\n\n\nCode\n# Calculate total spending by employment status\nemployment_spending = beer_data.groupby('employment').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nemployment_spending_sorted = employment_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by employment status\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='employment', data=employment_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Employment Status')\nplt.xlabel('Total Spending')\nplt.ylabel('Employment Status')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization shows the total spending on beer by employment status. We can see that certain employment groups, such as full-time workers, are spending more on beer, which might be related to their disposable income.\nFinally, let‚Äôs look at spending by race to complete the demographic analysis.\n\n\nCode\n# Calculate total spending by race\nrace_spending = beer_data.groupby('race').agg({'dollar_spent':'sum'}).reset_index()\n\n# Sort by total spending\nrace_spending_sorted = race_spending.sort_values('dollar_spent', ascending=False)\n\n# Plotting total spending by race\nplt.figure(figsize=(10, 6))\nsns.barplot(x='dollar_spent', y='race', data=race_spending_sorted, palette='viridis')\nplt.title('Total Spending on Beer by Race')\nplt.xlabel('Total Spending')\nplt.ylabel('Race')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above indicates the total spending on beer broken down by race, highlighting which racial groups account for the most beer spending within the dataset. This could reflect both the demographics of the regions where the data was collected and cultural preferences regarding beer.\nNow, let‚Äôs proceed to the third analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "href": "posts/beer-markets/beer-markets.html#price-sensitivity",
    "title": "Beer Markets",
    "section": "Price Sensitivity",
    "text": "Price Sensitivity\nWe‚Äôll look at the price per fluid ounce and see if there are any trends or correlations with the quantity purchased or the brand popularity. To do this, we‚Äôll visualize how the price is sensitive to the quantity purchased by brand.\n\n\nCode\n# Ensure there's no entries with 0 for 'price_per_floz' or 'quantity' to avoid log(0) issues\nfiltered_data = beer_data[(beer_data['price_per_floz'] &gt; 0) & (beer_data['quantity'] &gt; 0)]\n\n# Calculate log values for both 'price_per_floz' and 'quantity'\nfiltered_data['log_price_per_floz'] = np.log(filtered_data['price_per_floz'])\nfiltered_data['log_quantity'] = np.log(filtered_data['quantity'])\n\n# Use seaborn to create a scatterplot with fitted lines, facetted by 'brand'\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', col='brand', col_wrap=4, height=3, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5}, aspect = .75)\n\n# Adjusting plot aesthetics\ng.set_titles(\"{col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nHere‚Äôs the scatterplot with fitted straight lines for the log of price_per_floz versus the log of quantity, facetted by brands. Each subplot represents a different brand, showing the relationship between these two logarithmic variables along with a fitted line to illustrate the trend within each brand‚Äôs data.\n\n\nCode\n# Adjust the facetting to split rows by 'brand' and columns by 'promo' for a more detailed comparative analysis\ng = sns.lmplot(data=filtered_data, x='log_quantity', y='log_price_per_floz', row='brand', col='promo', height=3, aspect=.75, line_kws={'color': 'red'}, scatter_kws={'alpha':0.5})\n\n# Adjusting plot aesthetics\ng.set_titles(\"Brand: {row_name}\\n Promo: {col_name}\")\ng.set_axis_labels(\"Log of Quantity\", \"Log of Price per Floz\")\nplt.subplots_adjust(top=0.9, wspace = .4, hspace = .4)\ng.fig.suptitle('Log of Price per Floz vs. Log of Quantity')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe scatterplot has been reorganized to split rows by brand and columns by promo status, offering a comprehensive view across different brands and their promotional status. Each subplot now provides a clear comparison of the log of price_per_floz versus the log of quantity for purchases made on promotion versus those that were not, across various beer brands.\nThis layout facilitates an easier comparison across brands and how promotion impacts the relationship between quantity and price per fluid ounce within each brand.\nLastly, let‚Äôs move to the fourth analysis:"
  },
  {
    "objectID": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "href": "posts/beer-markets/beer-markets.html#promotional-impact-on-quantity-purchased",
    "title": "Beer Markets",
    "section": "Promotional Impact on Quantity Purchased",
    "text": "Promotional Impact on Quantity Purchased\nWe‚Äôll assess the impact of promotions on the quantity of beer purchased. For this analysis, we can calculate the average quantity purchased with and without promotions and visualize the difference. We‚Äôll do this for each brand to see which brands are most affected by promotions.\nLet‚Äôs begin this analysis by looking at the average quantity purchased with and without promotions for each brand.\n\n\nCode\n# Calculate average quantity purchased with and without promotions for each brand\npromo_impact = beer_data.groupby(['brand', 'promo']).agg({'quantity':'mean'}).reset_index()\n\n# Pivot the data to have promo and non-promo side by side for each brand\npromo_impact_pivot = promo_impact.pivot(index='brand', columns='promo', values='quantity').reset_index()\npromo_impact_pivot.columns = ['brand', 'non_promo', 'promo']\n\n# Calculate the difference in average quantity purchased between promo and non-promo\npromo_impact_pivot['promo_impact'] = promo_impact_pivot['promo'] - promo_impact_pivot['non_promo']\n\n# Sort by the impact of promo\npromo_impact_pivot_sorted = promo_impact_pivot.sort_values('promo_impact', ascending=False)\n\n# Plotting the difference in average quantity purchased between promo and non-promo for each brand\nplt.figure(figsize=(12, 10))\nsns.barplot(x='promo_impact', y='brand', data=promo_impact_pivot_sorted, palette='viridis')\nplt.title('Impact of Promotions on Average Quantity Purchased by Brand')\nplt.xlabel('Difference in Average Quantity Purchased (Promo - Non-Promo)')\nplt.ylabel('Brand')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart illustrates the impact of promotions on the average quantity of beer purchased by brand. A positive value indicates that, on average, more beer is purchased when there is a promotion compared to when there isn‚Äôt. Some brands appear to be significantly more influenced by promotions, with customers buying more when the products are on sale or promotion.\nThis comprehensive analysis has provided insights into purchase patterns, demographic preferences, price sensitivity, and the impact of promotions on beer purchases."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "danl-rw/danl-210-project.html#esg_proj_2024_data",
    "href": "danl-rw/danl-210-project.html#esg_proj_2024_data",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "1. esg_proj_2024_data",
    "text": "1. esg_proj_2024_data\n\nThe esg_proj_2024_data DataFrame which provides a list of companies and associated information, including ESG scores.\n\n\nimport pandas as pd\nurl_2024 = \"https://bcdanl.github.io/data/esg_proj_2024_data.csv\"\nesg_proj_2024_data = pd.read_csv(url_2024)\n\n\n  \n\n\n\nVariable Description\n\nSymbol: a company‚Äôs ticker;\nCompany Name: a company name;\nSector: a sector a company belongs to;\nIndustry: an industry a company belongs to;\nCountry: a country a company belongs to;\nMarket_Cap: a company‚Äôs market capitalization as of December 20, 2024 (Source: Nasdaq‚Äôs Stock Screener).\n\nA company‚Äôs market capitalization is the value of the company that is traded on the stock market, calculated by multiplying the total number of shares by the present share price.\n\nIPO_Year: the year a company first went public by offering its shares to be traded on a stock exchange.\nTotal_ESG: The overall ESG (Environmental, Social, and Governance) risk score, summarizing the company‚Äôs exposure to ESG-related risks as of March 31, 2024. A lower score indicates lower risk.\nEnvironmental: The company‚Äôs exposure to environmental risks (e.g., emissions, energy use, environmental policy) as of March 31, 2024. A lower score indicates lower risk.\nSocial: The company‚Äôs exposure to social risks (e.g., labor practices, human rights, diversity, and customer relations) as of March 31, 2024. A lower score indicates lower risk.\nGovernance: The company‚Äôs exposure to governance-related risks (e.g., board structure, executive pay, shareholder rights, transparency) as of March 31, 2024. A lower score indicates lower risk.\nControversy: A score reflecting the severity of recent ESG-related controversies involving the company as of March 31, 2024. Higher scores typically indicate greater or more serious controversies."
  },
  {
    "objectID": "danl-rw/danl-210-project.html#esg_proj_2025",
    "href": "danl-rw/danl-210-project.html#esg_proj_2025",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "2. esg_proj_2025",
    "text": "2. esg_proj_2025\n\nThe esg_proj_2025 DataFrame provides a list of companies and associated information.\n\n\nurl_2025 = \"https://bcdanl.github.io/data/esg_proj_2025.csv\"\nesg_proj_2025 = pd.read_csv(url_2025)\n\n\n  \n\n\n\nVariable Description\n\nMarket_Cap: a company‚Äôs market capitalization as of March 29, 2025."
  },
  {
    "objectID": "danl-rw/danl-210-project.html#stock_history_2023",
    "href": "danl-rw/danl-210-project.html#stock_history_2023",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "3. stock_history_2023",
    "text": "3. stock_history_2023\n\nThe stock_history_2023 DataFrame contains daily historical stock market data for the year 2023.\n\n\nurl = \"https://bcdanl.github.io/data/stock_history_2023.csv\"\nstock_history_2023 = pd.read_csv(url)\n\n\n  \n\n\nVariable Description\n\nSymbol: Company‚Äôs stock ticker\nDate: Trading date\nYear: Trading year\nOpen: Opening price on the date\nHigh: Highest price on the date\nLow: Lowest price on the date\nClose: Closing price on the date\nVolume: Trading volume\nDividend: Cash dividend paid per share on the date (if any), as reported by Yahoo Finance\nStock_Split: The ratio of any stock split that occurred on the given date."
  },
  {
    "objectID": "danl-rw/danl-210-project.html#data-collection",
    "href": "danl-rw/danl-210-project.html#data-collection",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "1. Data Collection",
    "text": "1. Data Collection\nFor data collection, include only the companies that are common to both the esg_proj_2024_data and esg_proj_2025 DataFrames.\n\nScraping web data falls into a legal gray area. In the U.S., scraping publicly available information is not illegal, but it is not always clearly allowed either.\n\nMost companies do not go after individuals for minor or non-commercial violations of their Terms of Service (ToS). Still, if the scraping causes harm, it can lead to legal trouble.\n\nTips for Collecting Data from Yahoo! Finance:\n\nScrape at a reasonable and moderate rate. To avoid overloading servers, use time.sleep(random.uniform(5, y)) between page visits.\nThe method of explicit waits are not required, but they are helpful for ensuring elements load before scraping.\nBe aware that some companies may not have data available for Environmental, Social, or Governance Risk Scores, or Controversy Level.\nConsider starting with the following setup for Selenium web-scrapping\n\n\n# %%\n# =============================================================================\n# Setup libraries\n# =============================================================================\nimport time\nimport random\nimport pandas as pd\nimport os\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\n# %%\n# =============================================================================\n# Setup working directory\n# =============================================================================\nwd_path = 'PATHNAME_OF_YOUR_WORKING_DIRECTORY'\nos.chdir(wd_path)\n\n\n# %%\n# =============================================================================\n# Setup WebDriver with options\n# =============================================================================\noptions = Options()\noptions.add_argument(\"window-size=1400,1200\")  # Set window size\noptions.add_argument('--disable-blink-features=AutomationControlled')  # Prevent detection of automation by disabling blink features\noptions.page_load_strategy = 'eager'  # Load only essential content first, skipping non-critical resources\n\ndriver = webdriver.Chrome(options=options)\n\n\na. ESG Data\n\nFor each company in the esg_proj_2025 DataFrame, employ the Python selenium library to gather ESG Risk Ratings, along with the Controversy Level from the Sustainability section of each company‚Äôs webpage on Yahoo! Finance, such as:\n\nApple Inc.¬†(AAPL)\nMicrosoft Corporation (MSFT)\n\n\n\n\n\nb. Historical Stock Data\n\nFor each company found in both the esg_proj_2024_data and esg_proj_2025 DataFrames, employ the selenium library to retrieve:\n\nDaily stock prices from January 1, 2024, to March 31, 2025\n\ne.g., https://finance.yahoo.com/quote/MSFT/history/?p=MSFT&period1=1704067200&period2=1743446400\n1704067200 = January 1, 2024\n1743446400 = March 31, 2025\n\n\nNote: GOOGLEFINANCE function in Google Sheets is freely available for retrieving current or historical stock data.\n\nAlthough our course does not cover Google Sheets, you are welcome to use it to collect historical stock data if you prefer.\nIf you choose Google Sheets‚Äô GOOGLEFINANCE() for collecting historical stock data, please share your Google Sheets with Prof.¬†Choe.\n\n\n\n\nDividend Cleaning\nIf you scrape historical data tables from each company‚Äôs page on Yahoo Finance (e.g., MSFT Historical Data), you can construct a DataFrame similar to the df_all DataFrame shown below.\nThe df_all DataFrame contains stock data for Apple Inc., Microsoft, and Nvidia from the beginning of 2024 through the end of Q1 2025:\nimport pandas as pd\ndf_all = pd.read_csv('https://bcdanl.github.io/data/aapl_msft_nvda_2024_2025.csv')\n\n  \n\nNote that some rows in the df_all DataFrame include dividend declarations rather than price and volume data (e.g., Apple Inc.¬†on February 10, 2025; Microsoft on February 20, 2025). These dividend entries appear in the Open/High/Low/Close/Adj Close/Volume columns as strings like ‚Äú0.25 Dividend‚Äù.\n\nNote: Apple Inc‚Äôs ‚Äú0.25 Dividend‚Äù means that on that specific date, Apple Inc issued a cash dividend of $0.01 per share.\n\nTo separate these dividend entries from the actual stock trading data, we use the str.contains() method:\n# Filter rows where the 'Open' column contains the word 'Dividend' (these represent dividend entries)\ndf_dividend = df_all[df_all['Open'].str.contains('Dividend', na=False)]\n\n# Filter out dividend rows to keep only stock price data\ndf_stock = df_all[~df_all['Open'].str.contains('Dividend', na=False)]\n\nAt this point:\n\ndf_stock does not include dividend announcements.\ndf_dividend includes only dividend announcements.\n\n\nWe now clean and format the dividend information:\n# Select only relevant columns for dividend data\ndf_dividend = df_dividend[['Date', 'Symbol', 'Open']]\n\n# Copy 'Open' column (which contains dividend information) into a new column named 'Dividend'\ndf_dividend['Dividend'] = df_dividend['Open']\n\n# Keep only the necessary columns: Date, Symbol, and Dividend\ndf_dividend = df_dividend[['Date', 'Symbol', 'Dividend']]\n\n# Remove the text \" Dividend\" from the Dividend column to isolate the numeric value\ndf_dividend['Dividend'] = df_dividend['Dividend'].str.replace(' Dividend', '')\n\n\n\nStock Split Cleaning\nSimilarly, some row in the df_stock DataFrame from the ‚ÄúDividend Cleaning‚Äù subsection includes stock splits rather than price and volume data (e.g., Nvidia on June 10, 2024). These stock split entries appear in the Open/High/Low/Close/Adj Close/Volume columns as strings like ‚Äú10:1 Stock Splits‚Äù.\nTo separate these stock split entries from the actual stock trading data, again we use the str.contains() method:\n# Filter rows where the 'Open' column contains the word 'Split' (these represent stock split entries)\ndf_split = df_stock[df_stock['Open'].str.contains('Split', na=False)]\n\n# Filter out dividend rows to keep only stock price data\ndf_stock = df_stock[~df_stock['Open'].str.contains('Split', na=False)]\n\nAt this point:\n\ndf_stock includes only daily stock trading information.\ndf_split includes only stock splits.\n\n\nWe now clean and format the split information:\n# Select only relevant columns for dividend data\ndf_split = df_split[['Date', 'Symbol', 'Open']]\n\n# Copy 'Open' column (which contains dividend information) into a new column named 'Split'\ndf_split['Split'] = df_split['Open']\n\n# Keep only the necessary columns: Date, Symbol, and Split\ndf_split = df_split[['Date', 'Symbol', 'Split']]\n\n# Remove the text \" Stock Splits\" from the Split column to isolate the numeric value\ndf_split['Split'] = df_split['Split'].str.replace(' Stock Splits', '')\n\nNote: NVDA‚Äôs ‚Äú10:1 Stock Split‚Äù means that each existing share was split into 10 shares.\n\nIf you owned 1 share before June 10, 2024, you would own 10 shares after the split.\nTo maintain consistency, Yahoo Finance retroactively adjusts all historical prices and volumes to reflect stock splits.\n\nFor example, the table below shows NVDA‚Äôs adjusted stock prices and volumes from June 7‚Äì11, 2024:\n\n\n  \n\n\n\n\nExtracting the year from a datetime variable\n\nTo extract the year from a datetime variable in a pandas DataFrame, you can use the .dt.year accessor.\n\n# Sample DataFrame with string dates\ndata = {\n    'Symbol': ['AAPL', 'MSFT', 'GOOG'],\n    'Date': ['2024-12-29', '2024-12-30', '2025-01-03'],\n    'Close': [130.21, 265.78, 122.34]\n}\n\ndf = pd.DataFrame(data)\n\n# Convert 'Date' column to datetime\ndf['Date'] = df['Date'].astype('datetime64[ns]')\n\n# Extract year from 'Date' variable\ndf['Year'] = df['Date'].dt.year"
  },
  {
    "objectID": "danl-rw/danl-210-project.html#data-analysis",
    "href": "danl-rw/danl-210-project.html#data-analysis",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "2. Data Analysis",
    "text": "2. Data Analysis\n\nIf you are unable to complete the data collection tasks, please use the esg_proj_2024_data and stock_history_2023 DataFrames for your data analysis.\nIf you successfully completed the data collection tasks, you may ‚Äúoptionally‚Äù incorporate the stock_history_2023 DataFrame into your analysis as an additional data source.\nBelow are the key components in the data analysis webpage.\n\nTitle: A clear and concise title that gives an idea of the project topics.\nIntroduction:\n\nBackground: Provide context for the research questions, explaining why they are significant, relevant, or interesting.\nStatement of the Problem: Clearly articulate the specific problem or issue the project will address.\n\nData Collection: Use a Python script (*.py) to write the code and the comment on how to retrieve ESG data and historical stock data using Python selenium.\n\nDo NOT provide your code for data collection in your webpage. You should submit your Python script for data collection to Brightspace.\n\nDescriptive Statistics\n\n\nProvide both grouped and un-grouped descriptive statistics and distribution plots for the ESG data and the finance/accounting data\nOptionally, provide correlation heat maps using corr() and seaborn.heatmap(). Below provides the Python code for creating a correlation heatmap.\n\n\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Sample DataFrame with varied correlations\ndata = {\n    'Revenue': [100, 200, 300, 400, 500],  \n    'Profit': [20, 40, 60, 80, 100],       \n    'n_Employee': [50, 45, 40, 35, 30], \n    'n_Customer': [10, 11, 12, 13, 14]  \n}\n\n# Create a DataFrame from the dictionary\ndf = pd.DataFrame(data)\n\n# Calculate the correlation matrix of the DataFrame\ncorr = df.corr()\n\n# Set up the matplotlib figure size\nplt.figure(figsize=(8, 6))\n\n# Generate a heatmap in seaborn:\n# - 'corr' is the correlation matrix\n# - 'annot=True' enables annotations inside the squares with the correlation values\n# - 'cmap=\"coolwarm\"' assigns a color map from cool to warm (blue to red)\n# - 'fmt=\".2f\"' formats the annotations to two decimal places\n# - 'linewidths=.5' adds lines between each cell\nsns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n\n# Title of the heatmap\nplt.title('Correlation Heatmap with Varied Correlations')\n\n# Display the heatmap\nplt.show()\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis:\n\nExplore the trend of ESG scores from 2024 to 2025.\nAdditionally, list the questions you aim to answer.\nAddress the questions through data visualization with seaborn (or lets-plot) and pandas methods and attributes.\n\nSignificance of the Project:\n\nExplain its implications for real-world applications, business strategies, or public policy.\n\nReferences\n\nList all sources cited in the project.\nLeave a web address of the reference if that is from the web.\nIndicate if the code and the write-up are guided by generative AI, such as ChatGPT. There will be no penalties on using any generative AI.\nClearly state if the code and the write-up result from collaboration with colleagues. There will be no penalties for collaboration, provided that the shared portions are clearly indicated."
  },
  {
    "objectID": "danl-rw/danl-210-project.html#interpreting-esg-data-on-yahoo-finance",
    "href": "danl-rw/danl-210-project.html#interpreting-esg-data-on-yahoo-finance",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "Interpreting ESG Data on Yahoo Finance üßæ",
    "text": "Interpreting ESG Data on Yahoo Finance üßæ\nIn Yahoo Finance, the ESG data helps investors evaluate a company‚Äôs sustainability profile and exposure to long-term environmental, social, and governance risks. Here‚Äôs how to interpret each metric:\n\nüî¢ Total ESG Risk Score\n\nWhat it means: A composite score reflecting the company‚Äôs overall exposure to ESG-related risks.\nHow to interpret:\n\nLower score = lower risk ‚Üí Better ESG performance.\n\nHigher score = higher risk ‚Üí More vulnerable to ESG-related issues.\n\nExample: A company with total_ESG = 15 is considered to have lower ESG risk than one with total_ESG = 30.\n\n\n\n\nüåç Environmental Risk Score\n\nWhat it measures: Exposure to environmental risks such as:\n\nCarbon emissions\n\nEnergy efficiency\n\nWaste management\n\nClimate change strategy\n\n\nInterpretation:\n\nLower score ‚Üí better environmental practices.\nHigher score ‚Üí more environmental liabilities or poor sustainability measures.\n\n\n\n\nüë• Social Risk Score\n\nWhat it measures: Exposure to social risks, including:\n\nLabor practices\n\nHuman rights\n\nInclusive culture and representation\nCustomer and community relations\n\n\nInterpretation:\n\nLower score = better social responsibility.\nHigher score = more risk from labor issues, PR problems, etc.\n\n\n\n\nüèõ Governance Risk Score\n\nWhat it measures: Exposure to governance-related risks, such as:\n\nBoard structure and independence\n\nExecutive compensation\n\nShareholder rights\n\nTransparency and ethics\n\n\nInterpretation:\n\nLower score suggests better corporate oversight.\nHigher score suggests poor governance structures.\n\n\n\n\nüö® Controversy Level\n\nWhat it measures: Reflects recent ESG-related controversies involving the company.\nScale: Usually ranges from 0 (no controversies) to 5 (severe and ongoing issues).\nInterpretation:\n\nLow score (0‚Äì1): Minimal or no controversies.\nHigh score (4‚Äì5): Major controversies ‚Äî potential reputational or legal risks.\nNote: A company may have good ESG scores but still be flagged due to a high controversy score.\n\n\n\n\nüß† ESG Score Summary\n\n\n\nMetric\nGood Score\nBad Score\n\n\n\n\ntotal_ESG\nLow\nHigh\n\n\nEnvironmental\nLow\nHigh\n\n\nSocial\nLow\nHigh\n\n\nGovernance\nLow\nHigh\n\n\nControversy\n0‚Äì1\n4‚Äì5"
  },
  {
    "objectID": "danl-rw/danl-210-project.html#general-tips-on-data-visualization",
    "href": "danl-rw/danl-210-project.html#general-tips-on-data-visualization",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "General Tips on Data Visualization üìà",
    "text": "General Tips on Data Visualization üìà\n\nDistribution\nWhen describing the distribution of a variable, we are typically interested in several key characteristics:\n\nCenter: The central tendency of the data, such as the mean or median, which indicates the typical or average value.\nSpread: How spread the values are within the variable, showing the range and standard deviation of values.\nCommon Values: Identifying frequent values and the mode.\nRare Values: Recognizing unusual or infrequent values.\nShape: The overall shape of the distribution, such as whether it‚Äôs symmetric, skewed left or right, or having multiple groups with multiple peaks.\n\n\n\n\n\n\n\n\n\nRelationship Between Two Variables\n\nStart with determining whether the two variables have a positive association, a negative association, or no association.\n\n\nE.g., A negative slope in the fitted line indicates that sales decrease as the price increases, while a positive slope would indicate that sales increase with price. A zero slope means that there is no relationship between sales and price; changes in price do not affect sales.\n\n\n\n\n\n\n\n\nInput on the x-axis; output on the y-axis\n\n\nBy convention, the input (or predictor) variable is plotted on the x-axis, and the output (or response) variable on the y-axis.\nThis helps visualize potential relationships‚Äîthough it shows correlation, not necessarily causation.\nCorrelation does not necessarily mean causation.\n\n\nWhen a question asks you to describe how the relationship varies by another categorical variable, examine both the direction of the slope (negative, positive, or none) from the fitted line and the steepness of the slope (steep or shallow).\n\n\nThe slope of the fitted straight line is the rate at which the ‚Äúy‚Äù variable (like grades) changes as the ‚Äúx‚Äù variable (like study hours) changes. In simple terms, it shows how much one thing goes up or down when the other thing changes.\nFor example, a comment such as, ‚ÄúThe plot shows a negative relationship between sales and price‚Äù does not address how the relationship differs by brand.\n\n\nThe focus is on the relationship, not the distribution.\n\n\nWhile adding a comment on the distribution of a single variable can be helpful, the question is primarily about the relationship between the two variables.\n\n\n\n\nTime Trend of a Variable\nHere are some general tips for describing the time trend of a variable:\n\nStart with Identifying the Overall Trend\n\n\nLook for the general direction of the trend over time.\n\nIs it moving upward, downward, or remaining relatively constant?\n\n\n\nNote Patterns and Cycles\n\n\nIdentify any repeating patterns, such as seasonal fluctuations (e.g., monthly or quarterly changes) or long-term cycles.\n\nThese can reveal consistent influences that affect the variable over time.\n\n\n\nHighlight Any Significant Fluctuations\n\n\nDescribe any sharp increases, decreases, or irregular spikes in the data.\n\n\n\n\n\n\n\n\n\nInterpreting Visualization\n\nBe specific.\n\nAvoid vague statements. Below examples do not actually explain what the patterns are.\n\n‚ÄúThe plot shows how the time trend of a stock price varies across sectors, with each sector having a unique best fitting line and scatter pattern‚Äù\n‚ÄúThe trend shows the evolution of stock price in the market over time‚Äù\n\nClearly describe what is the pattern‚Äîand how it differs across categories.\n\nAdd Narration:\n\nConnect the visualization to real-world phenomena and/or your idea that could help explain it, adding insight into what is happening."
  },
  {
    "objectID": "danl-rw/danl-210-project.html#project-write-up",
    "href": "danl-rw/danl-210-project.html#project-write-up",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "Project Write-up",
    "text": "Project Write-up\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nVery Deficient (1)\nSomewhat Deficient (2)\nAcceptable (3)\nVery Good (4)\nOutstanding (5)\n\n\n\n\n1. Quality of research questions\n‚Ä¢ Not stated or very unclear‚Ä¢ Entirely derivative‚Ä¢ Anticipate no contribution\n‚Ä¢ Stated somewhat confusingly‚Ä¢ Slightly interesting, but largely derivative‚Ä¢ Anticipate minor contributions\n‚Ä¢ Stated explicitly‚Ä¢ Somewhat interesting and creative‚Ä¢ Anticipate limited contributions\n‚Ä¢ Stated explicitly and clearly‚Ä¢ Clearly interesting and creative‚Ä¢ Anticipate at least one good contribution\n‚Ä¢ Articulated very clearly‚Ä¢ Highly interesting and creative‚Ä¢ Anticipate several important contributions\n\n\n2. Quality of data visualization\n‚Ä¢ Very poorly visualized‚Ä¢ Unclear‚Ä¢ Unable to interpret figures\n‚Ä¢ Somewhat visualized‚Ä¢ Somewhat unclear‚Ä¢ Difficulty interpreting figures\n‚Ä¢ Mostly well visualized‚Ä¢ Mostly clear‚Ä¢ Acceptably interpretable\n‚Ä¢ Well organized‚Ä¢ Well thought-out visualization‚Ä¢ Almost all figures clearly interpretable\n‚Ä¢ Very well visualized‚Ä¢ Outstanding visualization‚Ä¢ All figures clearly interpretable\n\n\n3. Quality of exploratory data analysis\n‚Ä¢ Little or no critical thinking‚Ä¢ Little or no understanding of data analytics concepts with Python\n‚Ä¢ Rudimentary critical thinking‚Ä¢ Somewhat shaky understanding of data analytics concepts with Python\n‚Ä¢ Average critical thinking‚Ä¢ Understanding of data analytics concepts with Python\n‚Ä¢ Mature critical thinking‚Ä¢ Clear understanding of data analytics concepts with Python\n‚Ä¢ Sophisticated critical thinking‚Ä¢ Superior understanding of data analytics concepts with Python\n\n\n4. Quality of business/economic analysis\n‚Ä¢ Little or no critical thinking‚Ä¢ Little or no understanding of business/economic concepts\n‚Ä¢ Rudimentary critical thinking‚Ä¢ Somewhat shaky understanding of business/economic concepts\n‚Ä¢ Average critical thinking‚Ä¢ Understanding of business/economic concepts\n‚Ä¢ Mature critical thinking‚Ä¢ Clear understanding of business/economic concepts\n‚Ä¢ Sophisticated critical thinking‚Ä¢ Superior understanding of business/economic concepts\n\n\n5. Quality of writing\n‚Ä¢ Very poorly organized‚Ä¢ Very difficult to read‚Ä¢ Many typos and grammatical errors\n‚Ä¢ Somewhat disorganized‚Ä¢ Somewhat difficult to read‚Ä¢ Numerous typos and grammatical errors\n‚Ä¢ Mostly well organized‚Ä¢ Mostly easy to read‚Ä¢ Some typos and grammatical errors\n‚Ä¢ Well organized‚Ä¢ Easy to read‚Ä¢ Very few typos or grammatical errors\n‚Ä¢ Very well organized‚Ä¢ Very easy to read‚Ä¢ No typos or grammatical errors\n\n\n6. Quality of Jupyter Notebook usage\n‚Ä¢ Very poorly organized‚Ä¢ Many redundant warning/error messages‚Ä¢ Inappropriate code to produce outputs\n‚Ä¢ Somewhat disorganized‚Ä¢ Numerous warning/error messages‚Ä¢ Misses important code\n‚Ä¢ Mostly well organized‚Ä¢ Some warning/error messages‚Ä¢ Provides appropriate code\n‚Ä¢ Well organized‚Ä¢ Very few warning/error messages‚Ä¢ Provides advanced code\n‚Ä¢ Very well organized‚Ä¢ No warning/error messages‚Ä¢ Proposes highly advanced code"
  },
  {
    "objectID": "danl-rw/danl-210-project.html#data-collection-1",
    "href": "danl-rw/danl-210-project.html#data-collection-1",
    "title": "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis",
    "section": "Data Collection",
    "text": "Data Collection\n\n\n\n\n\n\n\n\n\nEvaluation\nDescription\nCriteria\n\n\n\n\n1 (Very Deficient)\n- Very poorly implemented- Data is unreliable.\n- Poor web scraping practices with selenium, leading to unreliable or incorrect data from Yahoo Finance.- Inadequate use of pandas, resulting in poorly structured DataFrames.\n\n\n2 (Somewhat Deficient)\n- Somewhat effective implementation- Data has minor reliability issues.\n- Basic web scraping with selenium that sometimes fails to capture all relevant data accurately.- Basic use of pandas, but with occasional issues in data structuring.\n\n\n3 (Acceptable)\n- Effective web scraping with selenium, capturing most required data from Yahoo Finance.- Adequate use of pandas to structure data in a mostly logical format.\n\n\n\n4 (Very Good)\n- Well-implemented and organized- Data is reliable.\n- Thorough web scraping with selenium that consistently captures accurate and complete data from Yahoo Finance.- Skillful use of pandas for clear and logical data structuring.\n\n\n5 (Outstanding)\n- Exceptionally implemented- Data is highly reliable.\n- Expert web scraping with selenium, capturing detailed and accurate data from Yahoo Finance without fail.- Expert use of pandas to create exceptionally well-organized DataFrames that facilitate easy analysis."
  },
  {
    "objectID": "listing-danl-210-ex.html",
    "href": "listing-danl-210-ex.html",
    "title": "DANL 210 - Exam",
    "section": "",
    "text": "Title\nSubtitle\nDate\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "posts/nba/nba.html#salary-distribution-among-teams",
    "href": "posts/nba/nba.html#salary-distribution-among-teams",
    "title": "NBA",
    "section": "Salary Distribution Among Teams",
    "text": "Salary Distribution Among Teams\nLet‚Äôs start with the salary distribution among teams using seaborn for visualization. ‚Äã‚Äã\n\n\nCode\n\n# Handle missing values in 'Salary' by replacing them with the median salary\nmedian_salary = nba['Salary'].median()\nnba['Salary'].fillna(median_salary, inplace=True)\n\n\n\n\nCode\n# Set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Calculate total salary by team\nteam_salary = (\n    nba\n    .groupby('Team')['Salary']\n    .sum()\n    .reset_index()\n    .sort_values(by='Salary', ascending=False)\n)\n\n# Plot total salary by team\nplt.figure(figsize=(10, 8))\nsns.barplot(data = team_salary,\n            x = 'Salary', y = 'Team',\n            palette = 'coolwarm')\nplt.title('Total Salary Distribution Among NBA Teams')\nplt.xlabel('Total Salary')\nplt.ylabel('Team')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe visualization above displays the total salary distribution among NBA teams, with teams sorted by their total salary expenditure. This bar plot reveals which teams are the biggest spenders on player salaries and which are more conservative. The color gradient provides a visual cue to easily distinguish between the higher and lower spending teams. Portland Trail Blazers spent most in their players‚Äô salary, followed by Golden State Warriors and Philadelphia 76ers."
  },
  {
    "objectID": "posts/nba/nba.html#player-age-distribution",
    "href": "posts/nba/nba.html#player-age-distribution",
    "title": "NBA",
    "section": "Player Age Distribution",
    "text": "Player Age Distribution\nNext, let‚Äôs explore the Player Age Distribution across the NBA. We‚Äôll create a histogram to visualize how player ages are distributed, which will help us understand if the league trends younger, older, or has a balanced age mix. ‚Äã‚Äã\n\n\nCode\n# Convert 'Birthday' column to datetime format\nfrom dateutil import parser\nnba['Birthday'] = nba['Birthday'].apply(lambda x: parser.parse(x))\n\n# Now, let's calculate the age of each player\nnba['Age'] = (datetime.now() - nba['Birthday']).dt.days // 365\n\n# Plot the age distribution of NBA players\nplt.figure(figsize=(10, 6))\nsns.histplot(nba['Age'],\n             bins = 15,\n             kde = True,\n             color = 'skyblue')\nplt.title('Age Distribution of NBA Players')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe histogram above shows the age distribution of NBA players, with a kernel density estimate (KDE) overlay to indicate the distribution shape. The majority of players fall within a certain age range from 25 to 35, illustrating the league‚Äôs age dynamics. The plot helps identify the common ages for NBA players and whether there are significant numbers of very young or older players."
  },
  {
    "objectID": "posts/nba/nba.html#position-wise-salary-insights",
    "href": "posts/nba/nba.html#position-wise-salary-insights",
    "title": "NBA",
    "section": "Position-wise Salary Insights",
    "text": "Position-wise Salary Insights\nMoving on to Position-wise Salary Insights, we‚Äôll examine how average salaries differ across player positions. This analysis could reveal which positions are typically higher-paid, potentially reflecting their value on the basketball court. Let‚Äôs create a box plot to visualize the salary distribution for each position. ‚Äã‚Äã\n\n\nCode\n# Plot salary distribution by player position\nplt.figure(figsize=(10, 6))\nsns.boxplot(data = nba,\n            x = 'Position', y = 'Salary',\n            palette = 'Set2')\nplt.title('Salary Distribution by Position')\nplt.xlabel('Position')\nplt.ylabel('Salary')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe box plot above illustrates the salary distribution by player position, showcasing the variation in salaries among different positions within the NBA. This visualization helps us understand which positions tend to have higher median salaries and the spread of salaries within each position, including outliers that represent exceptionally high or low salaries. While the positions of C and PG have the widest interquantiles of salaries, the positions of FC, F, G, and GF have the narrowest interquantiles of them."
  },
  {
    "objectID": "posts/nba/nba.html#top-10-highest-paid-players",
    "href": "posts/nba/nba.html#top-10-highest-paid-players",
    "title": "NBA",
    "section": "Top 10 Highest Paid Players",
    "text": "Top 10 Highest Paid Players\nLastly, we‚Äôll identify the Top 10 Highest Paid Players in the NBA. This analysis highlights the star earners of the league, providing insights into which players command the highest salaries and potentially why. Let‚Äôs extract and visualize this information. ‚Äã‚Äã\n\n\nCode\n# Identify the top 10 highest paid players\ntop_10_salaries = nba.sort_values(by='Salary', ascending=False).head(10)\n\n# Plot the top 10 highest paid players\nplt.figure(figsize=(12, 8))\nsns.barplot(data = top_10_salaries,\n            x = 'Salary', y = 'Name',\n            palette = 'viridis')\nplt.title('Top 10 Highest Paid NBA Players')\nplt.xlabel('Salary')\nplt.ylabel('Player')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bar chart above reveals the top 10 highest-paid NBA players, showcasing those who stand at the pinnacle of the league in terms of salary. Stephen Curry is the highest-paid NBA player, followed by Russel Westbrook and Chris Paul. This visualization not only highlights the star players who command the highest salaries but also may reflect their marketability, performance, and contribution to their respective teams."
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html",
    "href": "posts/py-basic/blog-python-basics.html",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')\n\n\n\n\n\nIn Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5\n\n\n\n\n\nPython supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')\n\n\n\n\n\nA function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()\n\n\n\n\n\nA list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#what-is-python",
    "href": "posts/py-basic/blog-python-basics.html#what-is-python",
    "title": "Python Basics",
    "section": "",
    "text": "Python is a high-level, interpreted programming language. This is a simple Python code:\n\n\nCode\nprint('Hello, World!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "href": "posts/py-basic/blog-python-basics.html#variables-and-data-types",
    "title": "Python Basics",
    "section": "",
    "text": "In Python, variables can store data of different types without explicitly declaring the type.\nFor example:\n\n\nCode\ninteger_variable = 10\nstring_variable = 'Hello'\nfloat_variable = 10.5\n\nfloat_variable\n\n\n10.5"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#control-structures",
    "href": "posts/py-basic/blog-python-basics.html#control-structures",
    "title": "Python Basics",
    "section": "",
    "text": "Python supports the usual logical conditions from mathematics:\n\n\nCode\n# Equals: a == b\n# Not Equals: a != b\n# Less than: a &lt; b\n# Less than or equal to: a &lt;= b\n# Greater than: a &gt; b\n# Greater than or equal to: a &gt;= b\n\n\nThese conditions can be used in several ways, most commonly in ‚Äòif statements‚Äô and loops.\n\n\nCode\n# if statement:\nif 5 &gt; 2:\n    print('Five is greater than two!')"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#functions",
    "href": "posts/py-basic/blog-python-basics.html#functions",
    "title": "Python Basics",
    "section": "",
    "text": "A function is a block of code which only runs when it is called.\nYou can pass data, known as parameters, into a function.\nA function can return data as a result.\n\n\nCode\n# Defining a function:\ndef my_function():\n    print('Hello from a function')\n\n# Calling a function:\nmy_function()"
  },
  {
    "objectID": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "href": "posts/py-basic/blog-python-basics.html#lists-and-dictionaries",
    "title": "Python Basics",
    "section": "",
    "text": "A list is a collection which is ordered and changeable.\nA dictionary is a collection which is unordered, changeable and indexed.\n\n\nCode\n# List example:\nmy_list = ['apple', 'banana', 'cherry']\n\n# Dictionary example:\nmy_dict = {'name': 'John', 'age': 36}"
  },
  {
    "objectID": "listing-danl-210-rw.html",
    "href": "listing-danl-210-rw.html",
    "title": "DANL 210 - Project",
    "section": "",
    "text": "Title\n\n\n\nDate\n\n\n\n\n\n\n\n\nUnifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis\n\n\nMay 12, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DANL 210-01: Data Preparation and Management, Fall 2025",
    "section": "",
    "text": "Welcome! üëã\n\\(-\\) Explore, Learn, and Grow with Data Analytics! üåü"
  },
  {
    "objectID": "index.html#bullet-lecture-slides",
    "href": "index.html#bullet-lecture-slides",
    "title": "DANL 210-01: Data Preparation and Management, Fall 2025",
    "section": "\\(\\bullet\\,\\) Lecture Slides üöÄ",
    "text": "\\(\\bullet\\,\\) Lecture Slides üöÄ\n\n\n\n\nTitle\nSubtitle\nDate\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-classwork",
    "href": "index.html#bullet-classwork",
    "title": "DANL 210-01: Data Preparation and Management, Fall 2025",
    "section": "\\(\\bullet\\,\\) Classwork ‚å®Ô∏è",
    "text": "\\(\\bullet\\,\\) Classwork ‚å®Ô∏è\n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nClasswork 1\n\n\nBuilding a Personal Website using Git, GitHub, and RStudio with Quarto\n\n\nJanuary 24, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-homework",
    "href": "index.html#bullet-homework",
    "title": "DANL 210-01: Data Preparation and Management, Fall 2025",
    "section": "\\(\\bullet\\,\\) Homework üíª",
    "text": "\\(\\bullet\\,\\) Homework üíª\n\n\n\n\nTitle\nSubtitle\nDate\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#bullet-exam",
    "href": "index.html#bullet-exam",
    "title": "DANL 210-01: Data Preparation and Management, Fall 2025",
    "section": "\\(\\bullet\\,\\) Exam üìù",
    "text": "\\(\\bullet\\,\\) Exam üìù\n\n\n\n\nTitle\nSubtitle\nDate\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "listing-danl-210-cw.html",
    "href": "listing-danl-210-cw.html",
    "title": "DANL 210 - Classwork",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nClasswork 1\n\n\nBuilding a Personal Website using Git, GitHub, and RStudio with Quarto\n\n\nJanuary 24, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "listing-danl-210-qa.html",
    "href": "listing-danl-210-qa.html",
    "title": "DANL 210 - Q & A",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nDANL 210 - Discussion and Q & A Board\n\n\n¬†\n\n\nJanuary 21, 2025\n\n\n\n\n\n\nNo matching items\n Back to top"
  }
]