---
title: "Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis"
subtitle: DANL 210 Project
author: Byeong-Hak Choe
institute: SUNY Geneseo
date: last-modified
format: 
  html
    # toc: true
    # toc-depth: 2
    # toc-expand: true
    # toc-title: Contents
code-fold: false

execute:
  echo: true
  eval: true
  message: false
  warning: false
  fig-width: 9
  fig-height: 6
---
```{r}
#| include: false
library(tidyverse)
library(skimr)
library(broom)
library(hrbrthemes)

theme_set(theme_ipsum() +
          theme(strip.background =element_rect(fill="lightgray"),
                axis.title.x = element_text(size = rel(1.5) ),
                axis.title.y = element_text(size = rel(1.5) ))
          )

reticulate::use_condaenv("/Users/bchoe/anaconda3", required = TRUE)

```

<br>

## Project

- Publish the webpage of your data analysis project on your website, hosted on GitHub. 
  - Your data analysis should focus on the agenda, **"Unifying Environmental, Social, and Governance (ESG) Metrics with Financial Analysis"**. 
  - Use either a Jupyter Notebook or a Quarto document to present your work.
  - The due for the project is May 16, 2025, Friday, 11:59 P.M.

<br>

## Project Data
- Below is the `esg_proj_2024_data` DataFrame and the `esg_proj_2025` DataFrame.
  - `esg_proj_2024_data` DataFrame which provides a list of companies and associated information, including ESG scores.
  - `esg_proj_2025` DataFrame provides a list of companies and associated information.

```{python}
#| eval: false
import pandas as pd
esg_proj_2024_data = pd.read_csv("https://bcdanl.github.io/data/esg_proj_2024_data.csv")
```

```{r}
#| echo: false
#| results: asis

esg_proj_2024_data <- read_csv("https://bcdanl.github.io/data/esg_proj_2024_data.csv")
DT::datatable(esg_proj_2024_data, rownames = F)
```

<br>

```{python}
#| eval: false
esg_proj_2025 = pd.read_csv("https://bcdanl.github.io/data/")
```

```{r}
#| echo: false
#| results: asis

esg_proj_2025 <- read_csv("https://bcdanl.github.io/data/esg_proj_2025.csv")
DT::datatable(esg_proj_2025, rownames = F)
```

#### Variable Description

- `Symbol`: a company's ticker;
- `Company Name`: a company name;
- `Sector`: a sector a company belongs to;
- `Industry`: an industry a company belongs to;
- `Country`: a country a company belongs to;
- `Market Cap`: a company's market capitalization as of December 20, 2024 (Source: [Nasdaq's Stock Screener](https://www.nasdaq.com/market-activity/stocks/screener)).
  - A company's market capitalization is the value of the company that is traded on the stock market, calculated by multiplying the total number of shares by the present share price.
- `IPO_Year`: the year a company first went public by offering its shares to be traded on a stock exchange.



<br><br>

## Project Tasks

### Data Collection
- For data collection, consider the `esg_proj_2025` DataFrame.

### `selenium`
- Employ the Python `selenium` library to gather ESG Risk Ratings, along with the Controversy Level from the Sustainability Section of each company's webpage on [Yahoo! Finance](https://finance.yahoo.com), such as:
  - [Agilent Technologies (A)](https://finance.yahoo.com/quote/A/sustainability)
  - [Alcoa Corporation (AA)](https://finance.yahoo.com/quote/AA/sustainability)
  
- Ensure robust error and exception handling during data collection from the Sustainability Section on [Yahoo! Finance](https://finance.yahoo.com):
  1. To avoid being blocked by [Yahoo! Finance](https://finance.yahoo.com) for automated browsing activities, please manage the Chrome driver by using  `time.sleep(random.uniform(x,y))` for each visit of the Sustainability Section webpage.
  2. Some companies may lack available data on Environmental Risk Score, Social Risk Score, Governance Risk Score, and/or Controversy Level.



### `yfinance`
- Utilize the `yfinance` library, an unofficial API for accessing [Yahoo! Finance](https://finance.yahoo.com) data.
  - For each company listed in the `esg_proj` DataFrame, you should retrieve the following:
    - Daily historical stock data spanning from January 1, 2024, to March 31, 2025.
    - Quarterly income statements covering the period from March 31, 2024, to March 31, 2025 (encompassing five quarters).
    - Quarterly balance sheets for the same period as the income statements.



<br>


### Data Analysis

- Below are the key components in the data analysis webpage.

  1. **Title**: A clear and concise title that gives an idea of the project topics.
  
  2. **Introduction**:
     - **Background**: Provide context for the research questions, explaining why they are significant, relevant, or interesting.
     - **Statement of the Problem**: Clearly articulate the specific problem or issue the project will address.

  3. **Data Collection**: Use a Python script (`*.py`) to write the code and the comment on how to retrieve financial, accounting, and ESG data using Python `yfinance` and `selenium`.
     - Do NOT provide your code for data collection in your webpage. You should submit your Python script for data collection to Brightspace.

  4. **Descriptive Statistics**
    - Provide both grouped and un-grouped descriptive statistics and distribution plots for the ESG data and the finance/accounting data
    - Provide correlation heat maps using `corr()` and `seaborn.heatmap()`. Below provides the Python code for creating a correlation heatmap.

```{python}
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample DataFrame with varied correlations
data = {
    'Revenue': [100, 200, 300, 400, 500],  
    'Profit': [20, 40, 60, 80, 100],       
    'n_Employee': [50, 45, 40, 35, 30], 
    'n_Customer': [10, 11, 12, 13, 14]  
}

# Create a DataFrame from the dictionary
df = pd.DataFrame(data)

# Calculate the correlation matrix of the DataFrame
corr = df.corr()

# Set up the matplotlib figure size
plt.figure(figsize=(8, 6))

# Generate a heatmap in seaborn:
# - 'corr' is the correlation matrix
# - 'annot=True' enables annotations inside the squares with the correlation values
# - 'cmap="coolwarm"' assigns a color map from cool to warm (blue to red)
# - 'fmt=".2f"' formats the annotations to two decimal places
# - 'linewidths=.5' adds lines between each cell
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

# Title of the heatmap
plt.title('Correlation Heatmap with Varied Correlations')

# Display the heatmap
plt.show()
```


  5. **Exploratory Data Analysis**:
     - Explore the trend of ESG scores from 2024 to 2025.
     - Additionally, list the questions you aim to answer.
     - Address the questions through data visualization with `seaborn` (or `lets-plot`) and `pandas` methods and attributes.
  
  6. **Significance of the Project**:
     - Explain its implications for real-world applications, business strategies, or public policy.
  
  7. **References**
     - List all sources cited in the project.
     - Leave a web address of the reference if that is from the web.
     - Indicate if the code and the write-up are guided by generative AI, such as ChatGPT. There will be no penalties on using any generative AI.
     - Clearly state if the code and the write-up result from collaboration with colleagues. There will be no penalties for collaboration, provided that the shared portions are clearly indicated.



<br>

## Rubric for the Project

| **Attribute** | **Very Deficient (1)** | **Somewhat Deficient (2)** | **Acceptable (3)** | **Very Good (4)** | **Outstanding (5)** |
|--------------|-------------------------|-----------------------------|--------------------|-------------------|----------------------|
| **1. Quality of research questions** | • Not stated or very unclear<br>• Entirely derivative<br>• Anticipate no contribution | • Stated somewhat confusingly<br>• Slightly interesting, but largely derivative<br>• Anticipate minor contributions | • Stated explicitly<br>• Somewhat interesting and creative<br>• Anticipate limited contributions | • Stated explicitly and clearly<br>• Clearly interesting and creative<br>• Anticipate at least one good contribution | • Articulated very clearly<br>• Highly interesting and creative<br>• Anticipate several important contributions |
| **2. Quality of data visualization** | • Very poorly visualized<br>• Unclear<br>• Unable to interpret figures | • Somewhat visualized<br>• Somewhat unclear<br>• Difficulty interpreting figures | • Mostly well visualized<br>• Mostly clear<br>• Acceptably interpretable | • Well organized<br>• Well thought-out visualization<br>• Almost all figures clearly interpretable | • Very well visualized<br>• Outstanding visualization<br>• All figures clearly interpretable |
| **3. Quality of exploratory data analysis** | • Little or no critical thinking<br>• Little or no understanding of data analytics concepts with Python | • Rudimentary critical thinking<br>• Somewhat shaky understanding of data analytics concepts with Python | • Average critical thinking<br>• Understanding of data analytics concepts with Python | • Mature critical thinking<br>• Clear understanding of data analytics concepts with Python | • Sophisticated critical thinking<br>• Superior understanding of data analytics concepts with Python |
| **4. Quality of business/economic analysis** | • Little or no critical thinking<br>• Little or no understanding of business/economic concepts | • Rudimentary critical thinking<br>• Somewhat shaky understanding of business/economic concepts | • Average critical thinking<br>• Understanding of business/economic concepts | • Mature critical thinking<br>• Clear understanding of business/economic concepts | • Sophisticated critical thinking<br>• Superior understanding of business/economic concepts |
| **5. Quality of writing** | • Very poorly organized<br>• Very difficult to read<br>• Many typos and grammatical errors | • Somewhat disorganized<br>• Somewhat difficult to read<br>• Numerous typos and grammatical errors | • Mostly well organized<br>• Mostly easy to read<br>• Some typos and grammatical errors | • Well organized<br>• Easy to read<br>• Very few typos or grammatical errors | • Very well organized<br>• Very easy to read<br>• No typos or grammatical errors |
| **6. Quality of Jupyter Notebook usage** | • Very poorly organized<br>• Many redundant warning/error messages<br>• Inappropriate code to produce outputs | • Somewhat disorganized<br>• Numerous warning/error messages<br>• Misses important code | • Mostly well organized<br>• Some warning/error messages<br>• Provides appropriate code | • Well organized<br>• Very few warning/error messages<br>• Provides advanced code | • Very well organized<br>• No warning/error messages<br>• Proposes highly advanced code |

<br>

### Rubric for Quality of Data Collection

| Evaluation | Description | Criteria |
|-------|-------------|----------|
| 1 (Very Deficient) | - Very poorly implemented<br>- Data is unreliable. | - Ineffective use of `yfinance`, resulting in incomplete or inaccurate financial data.<br>- Poor web scraping practices with `selenium`, leading to unreliable or incorrect data from Yahoo Finance.<br>- Inadequate use of `pandas`, resulting in poorly structured DataFrames. |
| 2 (Somewhat Deficient) | - Somewhat effective implementation<br>- Data has minor reliability issues. | - Basic use of `yfinance` with minor inaccuracies in data retrieval.<br>- Basic web scraping with `selenium` that sometimes fails to capture all relevant data accurately.<br>- Basic use of `pandas`, but with occasional issues in data structuring. |
| 3 (Acceptable) | - Competently implemented<br>- Data is mostly reliable. | - Competent use of `yfinance` to retrieve most financial data accurately.<br>- Effective web scraping with `selenium`, capturing most required data from Yahoo Finance.<br>- Adequate use of `pandas` to structure data in a mostly logical format. |
| 4 (Very Good) | - Well-implemented and organized<br>- Data is reliable. | - Advanced use of `yfinance` to reliably and accurately fetch financial data.<br>- Thorough web scraping with `selenium` that consistently captures accurate and complete data from Yahoo Finance.<br>- Skillful use of `pandas` for clear and logical data structuring. |
| 5 (Outstanding) | - Exceptionally implemented<br>- Data is highly reliable. | - Expert use of `yfinance` to obtain comprehensive and precise financial data.<br>- Expert web scraping with `selenium`, capturing detailed and accurate data from Yahoo Finance without fail.<br>- Expert use of `pandas` to create exceptionally well-organized DataFrames that facilitate easy analysis. |

